{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c5a1afd",
   "metadata": {},
   "source": [
    "# Perceptron as Minimizing Loss\n",
    "\n",
    "- ðŸ“º **Video:** [https://youtu.be/hhTkyP7EzGw](https://youtu.be/hhTkyP7EzGw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95aae7b",
   "metadata": {},
   "source": [
    "## Overview\n",
    "- Connect perceptron updates to subgradient descent on the hinge loss.\n",
    "- Understand when perceptron converges and how margins relate to mistake bounds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c0d3ef",
   "metadata": {},
   "source": [
    "## Key ideas\n",
    "- **Hinge loss:** upper-bounds the 0/1 loss and yields perceptron-like subgradients.\n",
    "- **Mistake bound:** perceptron converges in finite steps on linearly separable data with a margin.\n",
    "- **Update rule:** only mistaken examples trigger weight changes.\n",
    "- **Regularization:** variants like averaged perceptron smooth out oscillations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9132c2c",
   "metadata": {},
   "source": [
    "## Demo\n",
    "Simulate hinge-loss subgradient descent and the classic perceptron on the same dataset to show they share updates, matching the lecture walkthrough (https://youtu.be/hOUX9xFIN90)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20f6240d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T01:25:54.776832Z",
     "iopub.status.busy": "2025-10-29T01:25:54.776474Z",
     "iopub.status.idle": "2025-10-29T01:25:55.536610Z",
     "shell.execute_reply": "2025-10-29T01:25:55.536315Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  1 | perceptron mistakes  36 | accuracy 0.910\n",
      "epoch  2 | perceptron mistakes  31 | accuracy 0.940\n",
      "epoch  3 | perceptron mistakes  27 | accuracy 0.930\n",
      "epoch  4 | perceptron mistakes  31 | accuracy 0.923\n",
      "epoch  5 | perceptron mistakes  24 | accuracy 0.923\n",
      "epoch  6 | perceptron mistakes  28 | accuracy 0.937\n",
      "epoch  7 | perceptron mistakes  30 | accuracy 0.933\n",
      "epoch  8 | perceptron mistakes  23 | accuracy 0.923\n",
      "epoch  9 | perceptron mistakes  27 | accuracy 0.913\n",
      "epoch 10 | perceptron mistakes  24 | accuracy 0.913\n",
      "\n",
      "Final hinge weights close to perceptron weights? diff norm = 1.9347429944849273\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = make_classification(n_samples=300, n_features=4, n_informative=4, n_redundant=0, class_sep=1.5, random_state=5)\n",
    "y_signed = np.where(y == 1, 1, -1)\n",
    "X = np.c_[np.ones(len(X)), X]\n",
    "\n",
    "perceptron_w = np.zeros(X.shape[1])\n",
    "hinge_w = np.zeros_like(perceptron_w)\n",
    "eta = 0.1\n",
    "\n",
    "for epoch in range(10):\n",
    "    mistakes = 0\n",
    "    for xi, yi in zip(X, y_signed):\n",
    "        if yi * (perceptron_w @ xi) <= 0:\n",
    "            perceptron_w += eta * yi * xi\n",
    "            mistakes += 1\n",
    "        margin = yi * (hinge_w @ xi)\n",
    "        if margin < 1:\n",
    "            hinge_w += eta * yi * xi\n",
    "    preds = np.where((X @ perceptron_w) > 0, 1, -1)\n",
    "    acc = accuracy_score(y_signed, preds)\n",
    "    print(f\"epoch {epoch+1:2d} | perceptron mistakes {mistakes:3d} | accuracy {acc:.3f}\")\n",
    "\n",
    "print()\n",
    "print('Final hinge weights close to perceptron weights? diff norm =', np.linalg.norm(perceptron_w - hinge_w))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b579a8",
   "metadata": {},
   "source": [
    "## Try it\n",
    "- Modify the demo\n",
    "- Add a tiny dataset or counter-example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecc85b3",
   "metadata": {},
   "source": [
    "## References\n",
    "- [Eisenstein 2.0-2.5, 4.2-4.4.1](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Perceptron and logistic regression](https://www.cs.utexas.edu/~gdurrett/courses/online-course/perc-lr-connections.pdf)\n",
    "- [Eisenstein 4.1](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Perceptron and LR connections](https://www.cs.utexas.edu/~gdurrett/courses/online-course/perc-lr-connections.pdf)\n",
    "- [Thumbs up? Sentiment Classification using Machine Learning Techniques](https://www.aclweb.org/anthology/W02-1011/)\n",
    "- [Baselines and Bigrams: Simple, Good Sentiment and Topic Classification](https://www.aclweb.org/anthology/P12-2018/)\n",
    "- [Convolutional Neural Networks for Sentence Classification](https://www.aclweb.org/anthology/D14-1181/)\n",
    "- [[GitHub] NLP Progress on Sentiment Analysis](https://github.com/sebastianruder/NLP-progress/blob/master/english/sentiment_analysis.md)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81154283",
   "metadata": {},
   "source": [
    "*Links only; we do not redistribute slides or papers.*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
