{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74c662f7",
   "metadata": {},
   "source": [
    "# Perceptron\n",
    "\n",
    "- ðŸ“º **Video:** [https://youtu.be/tMGv5ZcuVP4](https://youtu.be/tMGv5ZcuVP4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e6224f",
   "metadata": {},
   "source": [
    "## Overview\n",
    "Delves into the Perceptron algorithm in detail. Building on the earlier introduction, this video likely walks step-by-step through how the perceptron iteratively learns a linear classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92379e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "random.seed(0)\n",
    "CI = os.environ.get('CI') == 'true'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf78ae0",
   "metadata": {},
   "source": [
    "## Key ideas\n",
    "- It might start with a simple dataset of text examples (with tokens as features) and an initial weight vector.\n",
    "- Then, it demonstrates the perceptron's loop: for each training example, predict using current weights, and if the prediction is wrong, update the weights by adding the feature vector for positive examples (or subtracting for negative) to move the decision boundary toward the correct side.\n",
    "- The video emphasizes the perceptron's intuitive idea: it mistake-drives updates - only adjusting when an error is made.\n",
    "- Over time, these corrections lead to a separating boundary (if one exists)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bebeef",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb14f803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiny logistic regression demo on synthetic data\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "X, y = make_classification(n_samples=800, n_features=10, random_state=0)\n",
    "Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "clf = LogisticRegression(max_iter=500).fit(Xtr, ytr)\n",
    "print(\"Accuracy:\", accuracy_score(yte, clf.predict(Xte)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6147cd40",
   "metadata": {},
   "source": [
    "## Try it\n",
    "- Modify the demo\n",
    "- Add a tiny dataset or counter-example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47058a96",
   "metadata": {},
   "source": [
    "## References\n",
    "- [Eisenstein 2.0-2.5, 4.2-4.4.1](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Perceptron and logistic regression](https://www.cs.utexas.edu/~gdurrett/courses/online-course/perc-lr-connections.pdf)\n",
    "- [Eisenstein 4.1](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Perceptron and LR connections](https://www.cs.utexas.edu/~gdurrett/courses/online-course/perc-lr-connections.pdf)\n",
    "- [Thumbs up? Sentiment Classification using Machine Learning Techniques](https://www.aclweb.org/anthology/W02-1011/)\n",
    "- [Baselines and Bigrams: Simple, Good Sentiment and Topic Classification](https://www.aclweb.org/anthology/P12-2018/)\n",
    "- [Convolutional Neural Networks for Sentence Classification](https://www.aclweb.org/anthology/D14-1181/)\n",
    "- [[GitHub] NLP Progress on Sentiment Analysis](https://github.com/sebastianruder/NLP-progress/blob/master/english/sentiment_analysis.md)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bed9cfb",
   "metadata": {},
   "source": [
    "*Links only; we do not redistribute slides or papers.*"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
