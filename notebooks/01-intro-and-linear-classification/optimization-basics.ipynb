{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "967f74ee",
   "metadata": {},
   "source": [
    "# Optimization Basics\n",
    "\n",
    "- ðŸ“º **Video:** [https://youtu.be/65ui-GdtY0Q](https://youtu.be/65ui-GdtY0Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef8e905",
   "metadata": {},
   "source": [
    "## Overview\n",
    "(This item appears to be a short wrap-up or transition, possibly summarizing the key points on optimization from this week.) It likely recaps the important ideas about how models are trained by defining a loss and using algorithms like gradient descent. It might also introduce a few practical concerns, like the difference between batch and stochastic gradient descent (SGD), or the idea of tuning hyperparameters (learning rate, number of iterations) for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7ab2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "random.seed(0)\n",
    "CI = os.environ.get('CI') == 'true'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0ac20c",
   "metadata": {},
   "source": [
    "## Key ideas\n",
    "- By reinforcing these â€œbasi the video ensures students are comfortable with the concept of model training as they head into Week 2, where neural networks will be introduced and rely heavily on these optimization principles.\n",
    "- Essentially, it cements the understanding that training = optimizing a loss over examples, a theme that will recur throughout the course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86960893",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4290a5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Try the exercises below and follow the linked materials.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc57466",
   "metadata": {},
   "source": [
    "## Try it\n",
    "- Modify the demo\n",
    "- Add a tiny dataset or counter-example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810045dc",
   "metadata": {},
   "source": [
    "## References\n",
    "- [Eisenstein 2.0-2.5, 4.2-4.4.1](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Perceptron and logistic regression](https://www.cs.utexas.edu/~gdurrett/courses/online-course/perc-lr-connections.pdf)\n",
    "- [Eisenstein 4.1](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Perceptron and LR connections](https://www.cs.utexas.edu/~gdurrett/courses/online-course/perc-lr-connections.pdf)\n",
    "- [Thumbs up? Sentiment Classification using Machine Learning Techniques](https://www.aclweb.org/anthology/W02-1011/)\n",
    "- [Baselines and Bigrams: Simple, Good Sentiment and Topic Classification](https://www.aclweb.org/anthology/P12-2018/)\n",
    "- [Convolutional Neural Networks for Sentence Classification](https://www.aclweb.org/anthology/D14-1181/)\n",
    "- [[GitHub] NLP Progress on Sentiment Analysis](https://github.com/sebastianruder/NLP-progress/blob/master/english/sentiment_analysis.md)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781a9a43",
   "metadata": {},
   "source": [
    "*Links only; we do not redistribute slides or papers.*"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
