{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "967f74ee",
   "metadata": {},
   "source": [
    "# Optimization Basics\n",
    "\n",
    "- ðŸ“º **Video:** [https://youtu.be/65ui-GdtY0Q](https://youtu.be/65ui-GdtY0Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef8e905",
   "metadata": {},
   "source": [
    "## Overview\n",
    "- Contrast batch, stochastic, and mini-batch optimization strategies for convex losses.\n",
    "- See how gradients, momentum, and adaptive schedules interact in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0ac20c",
   "metadata": {},
   "source": [
    "## Key ideas\n",
    "- **Batch vs. stochastic:** full gradients are stable but slow; SGD adds noise that can escape shallow minima.\n",
    "- **Mini-batching:** balances stability and efficiency.\n",
    "- **Momentum:** accumulates gradient history to smooth directions.\n",
    "- **Adaptive rates:** algorithms like Adagrad rescale steps per-parameter based on observed gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86960893",
   "metadata": {},
   "source": [
    "## Demo\n",
    "Compare batch gradient descent and stochastic gradient descent on the same logistic regression objective to visualize their convergence, tying back to the lecture (https://youtu.be/XOqa0dQDdJY)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4290a5d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T01:25:54.010255Z",
     "iopub.status.busy": "2025-10-29T01:25:54.010038Z",
     "iopub.status.idle": "2025-10-29T01:25:54.088247Z",
     "shell.execute_reply": "2025-10-29T01:25:54.087591Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  60 | batch loss 0.2694 | sgd loss 0.2830\n",
      "step 120 | batch loss 0.2051 | sgd loss 0.2096\n",
      "step 180 | batch loss 0.1753 | sgd loss 0.1770\n",
      "step 240 | batch loss 0.1571 | sgd loss 0.1615\n",
      "step 300 | batch loss 0.1445 | sgd loss 0.1476\n",
      "\n",
      "Batch weights: [ 0.11624997  0.85447811 -2.20080151  3.70600393]\n",
      "SGD weights: [ 0.19620556  0.89518683 -1.94777362  3.87739243]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "rng = np.random.default_rng(21)\n",
    "X = rng.normal(size=(400, 3))\n",
    "true_w = np.array([0.5, -1.2, 2.0])\n",
    "logits = X @ true_w\n",
    "probs = 1 / (1 + np.exp(-logits))\n",
    "y = (probs > 0.5).astype(int)\n",
    "X = np.c_[np.ones(len(X)), X]\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def logistic_grad(w, xi, yi):\n",
    "    pred = sigmoid(xi @ w)\n",
    "    return (pred - yi) * xi\n",
    "\n",
    "w_batch = np.zeros(X.shape[1])\n",
    "w_sgd = np.zeros_like(w_batch)\n",
    "\n",
    "for step in range(1, 301):\n",
    "    preds = sigmoid(X @ w_batch)\n",
    "    grad = (X.T @ (preds - y)) / len(X)\n",
    "    w_batch -= 0.2 * grad\n",
    "\n",
    "    xi = X[step % len(X)]\n",
    "    yi = y[step % len(X)]\n",
    "    w_sgd -= 0.2 * logistic_grad(w_sgd, xi, yi)\n",
    "\n",
    "    if step % 60 == 0:\n",
    "        batch_loss = -np.mean(y * np.log(preds + 1e-8) + (1 - y) * np.log(1 - preds + 1e-8))\n",
    "        sgd_preds = sigmoid(X @ w_sgd)\n",
    "        sgd_loss = -np.mean(y * np.log(sgd_preds + 1e-8) + (1 - y) * np.log(1 - sgd_preds + 1e-8))\n",
    "        print(f\"step {step:3d} | batch loss {batch_loss:.4f} | sgd loss {sgd_loss:.4f}\")\n",
    "\n",
    "print()\n",
    "print('Batch weights:', w_batch)\n",
    "print('SGD weights:', w_sgd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc57466",
   "metadata": {},
   "source": [
    "## Try it\n",
    "- Modify the demo\n",
    "- Add a tiny dataset or counter-example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810045dc",
   "metadata": {},
   "source": [
    "## References\n",
    "- [Eisenstein 2.0-2.5, 4.2-4.4.1](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Perceptron and logistic regression](https://www.cs.utexas.edu/~gdurrett/courses/online-course/perc-lr-connections.pdf)\n",
    "- [Eisenstein 4.1](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Perceptron and LR connections](https://www.cs.utexas.edu/~gdurrett/courses/online-course/perc-lr-connections.pdf)\n",
    "- [Thumbs up? Sentiment Classification using Machine Learning Techniques](https://www.aclweb.org/anthology/W02-1011/)\n",
    "- [Baselines and Bigrams: Simple, Good Sentiment and Topic Classification](https://www.aclweb.org/anthology/P12-2018/)\n",
    "- [Convolutional Neural Networks for Sentence Classification](https://www.aclweb.org/anthology/D14-1181/)\n",
    "- [[GitHub] NLP Progress on Sentiment Analysis](https://github.com/sebastianruder/NLP-progress/blob/master/english/sentiment_analysis.md)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781a9a43",
   "metadata": {},
   "source": [
    "*Links only; we do not redistribute slides or papers.*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
