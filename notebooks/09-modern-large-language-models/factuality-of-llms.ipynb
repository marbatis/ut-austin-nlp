{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59597712",
   "metadata": {},
   "source": [
    "# Factuality of LLMs\n",
    "\n",
    "- ðŸ“º **Video:** [https://youtu.be/bQZvmQUlqcs](https://youtu.be/bQZvmQUlqcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dcb339",
   "metadata": {},
   "source": [
    "## Overview\n",
    "Turns to the problem that LLMs often produce false or unsupported statements confidently (hallucinations) and describes research into evaluating and mitigating that It might start with examples of factual errors in summaries or answers (like the model making up a citation or mis-stating a detail). Yixin Liu et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8281d93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "random.seed(0)\n",
    "CI = os.environ.get('CI') == 'true'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b253bf18",
   "metadata": {},
   "source": [
    "## Key ideas\n",
    "- 2023 is cited (a work on grounding summaries with robust human eval possibly).\n",
    "- The video likely notes that as LMs aren't linked to a source of truth, they can say plausible-sounding but incorrect info.\n",
    "- So evaluating factuality is key, especially in tasks like summarization or QA.\n",
    "- They list some approaches: (1) NLI-based methods: e.g., check each statement's veracity by seeing if the source text entails it (the Summac model by Laban et al."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee20aff",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce395b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Try the exercises below and follow the linked materials.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66662c4",
   "metadata": {},
   "source": [
    "## Try it\n",
    "- Modify the demo\n",
    "- Add a tiny dataset or counter-example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0d1832",
   "metadata": {},
   "source": [
    "## References\n",
    "- [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
    "- [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)\n",
    "- [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/)\n",
    "- [Demystifying Prompts in Language Models via Perplexity Estimation](https://arxiv.org/abs/2212.04037)\n",
    "- [Calibrate Before Use: Improving Few-Shot Performance of Language Models](https://arxiv.org/abs/2102.09690)\n",
    "- [Holistic Evaluation of Language Models](https://arxiv.org/abs/2211.09110)\n",
    "- [Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?](https://arxiv.org/abs/2202.12837)\n",
    "- [In-context Learning and Induction Heads](https://arxiv.org/abs/2209.11895)\n",
    "- [Multitask Prompted Training Enables Zero-Shot Task Generalization](https://arxiv.org/abs/2110.08207)\n",
    "- [Scaling Instruction-Finetuned Language Models](https://arxiv.org/abs/2210.11416)\n",
    "- [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)\n",
    "- [[Website] Stanford Alpaca: An Instruction-following LLaMA Model](https://crfm.stanford.edu/2023/03/13/alpaca.html)\n",
    "- [Revisiting the Gold Standard: Grounding Summarization Evaluation with Robust Human Evaluation](https://arxiv.org/abs/2212.07981)\n",
    "- [WiCE: Real-World Entailment for Claims in Wikipedia](https://arxiv.org/abs/2303.01432)\n",
    "- [SummaC: Re-Visiting NLI-based Models for Inconsistency Detection in Summarization](https://arxiv.org/abs/2111.09525)\n",
    "- [FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation](https://arxiv.org/abs/2305.14251)\n",
    "- [RARR: Researching and Revising What Language Models Say, Using Language Models](https://arxiv.org/abs/2210.08726)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e47a29",
   "metadata": {},
   "source": [
    "*Links only; we do not redistribute slides or papers.*"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
