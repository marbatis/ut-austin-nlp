{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d6028af",
   "metadata": {},
   "source": [
    "# GPT-3\n",
    "\n",
    "- üì∫ **Video:** [https://youtu.be/jn41DLgnqek](https://youtu.be/jn41DLgnqek)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7802f84",
   "metadata": {},
   "source": [
    "## Overview\n",
    "Discusses GPT-3 (Generative Pre-trained Transformer 3), the 2020 OpenAI model that truly popularized LLM capabilities. The video outlines GPT-3's scale (175 billion parameters) and how it's an extension of the earlier GPT-2 architecture but vastly bigger and trained on more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e084db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "random.seed(0)\n",
    "CI = os.environ.get('CI') == 'true'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ce149c",
   "metadata": {},
   "source": [
    "## Key ideas\n",
    "- It explains GPT-3's breakthrough: it can perform many tasks via prompting alone - e.g., given a question in English, it can answer it; given a short story prompt, it can continue it; given a few examples of a translation, it can do more translations - all without fine-tuning.\n",
    "- This is introduced as unsupervised multitask learning or ‚Äúfew-shot learning in-context‚Äù.\n",
    "- The reading by Radford et al.\n",
    "- 2019 might be GPT-2's paper titled \"Language Models are Unsupervised Multitask Learners\" (the famous GPT-2 blog/paper) and Brown et al."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9466f4cf",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cb7021",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Try the exercises below and follow the linked materials.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809c01f4",
   "metadata": {},
   "source": [
    "## Try it\n",
    "- Modify the demo\n",
    "- Add a tiny dataset or counter-example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305ccfa1",
   "metadata": {},
   "source": [
    "## References\n",
    "- [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
    "- [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)\n",
    "- [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/)\n",
    "- [Demystifying Prompts in Language Models via Perplexity Estimation](https://arxiv.org/abs/2212.04037)\n",
    "- [Calibrate Before Use: Improving Few-Shot Performance of Language Models](https://arxiv.org/abs/2102.09690)\n",
    "- [Holistic Evaluation of Language Models](https://arxiv.org/abs/2211.09110)\n",
    "- [Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?](https://arxiv.org/abs/2202.12837)\n",
    "- [In-context Learning and Induction Heads](https://arxiv.org/abs/2209.11895)\n",
    "- [Multitask Prompted Training Enables Zero-Shot Task Generalization](https://arxiv.org/abs/2110.08207)\n",
    "- [Scaling Instruction-Finetuned Language Models](https://arxiv.org/abs/2210.11416)\n",
    "- [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)\n",
    "- [[Website] Stanford Alpaca: An Instruction-following LLaMA Model](https://crfm.stanford.edu/2023/03/13/alpaca.html)\n",
    "- [Revisiting the Gold Standard: Grounding Summarization Evaluation with Robust Human Evaluation](https://arxiv.org/abs/2212.07981)\n",
    "- [WiCE: Real-World Entailment for Claims in Wikipedia](https://arxiv.org/abs/2303.01432)\n",
    "- [SummaC: Re-Visiting NLI-based Models for Inconsistency Detection in Summarization](https://arxiv.org/abs/2111.09525)\n",
    "- [FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation](https://arxiv.org/abs/2305.14251)\n",
    "- [RARR: Researching and Revising What Language Models Say, Using Language Models](https://arxiv.org/abs/2210.08726)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb628dbc",
   "metadata": {},
   "source": [
    "*Links only; we do not redistribute slides or papers.*"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
