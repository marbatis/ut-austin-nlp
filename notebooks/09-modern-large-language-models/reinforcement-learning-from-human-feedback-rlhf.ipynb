{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98243538",
   "metadata": {},
   "source": [
    "# Reinforcement Learning from Human Feedback (RLHF)\n",
    "\n",
    "- ðŸ“º **Video:** [https://youtu.be/DwAdhx6GFh8](https://youtu.be/DwAdhx6GFh8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35630517",
   "metadata": {},
   "source": [
    "## Overview\n",
    "- Align models by fine-tuning with reward models trained on human preference data.\n",
    "- Understand the steps: supervised fine-tuning, reward learning, policy optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d16f7c",
   "metadata": {},
   "source": [
    "## Key ideas\n",
    "- **Preference data:** annotators rank responses for quality and safety.\n",
    "- **Reward model:** approximates human preferences from text.\n",
    "- **Policy optimization:** e.g., PPO maximizes reward while staying near the base model.\n",
    "- **Safety guardrails:** combine reward shaping with constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0551a428",
   "metadata": {},
   "source": [
    "## Demo\n",
    "Simulate preference comparisons, train a logistic reward model, and run a simple policy update, paralleling the lecture (https://youtu.be/0pLSyg7Z9hA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "012235b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T01:32:45.270525Z",
     "iopub.status.busy": "2025-10-29T01:32:45.270253Z",
     "iopub.status.idle": "2025-10-29T01:32:46.043097Z",
     "shell.execute_reply": "2025-10-29T01:32:46.042792Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward estimates: [1.88613449 0.58578553 1.85394001 0.20599333]\n",
      "Updated policy distribution: [0.34040625 0.17767664 0.33497052 0.14694659]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "responses = ['Concise answer with sources.', 'Vague answer with speculation.', 'Helpful step-by-step reasoning.', 'Off-topic joke.']\n",
    "features = np.array([\n",
    "    [1.0, 0.8],  # relevant, safe\n",
    "    [0.2, 0.4],\n",
    "    [0.9, 0.9],\n",
    "    [0.1, 0.1]\n",
    "])\n",
    "preferences = [(0, 1), (2, 1), (0, 3), (2, 3)]\n",
    "\n",
    "X, y = [], []\n",
    "for better, worse in preferences:\n",
    "    diff = features[better] - features[worse]\n",
    "    X.append(diff)\n",
    "    y.append(1)\n",
    "    X.append(-diff)\n",
    "    y.append(0)\n",
    "\n",
    "clf = LogisticRegression().fit(X, y)\n",
    "rewards = clf.decision_function(features)\n",
    "print('Reward estimates:', rewards)\n",
    "\n",
    "policy = np.array([0.25, 0.25, 0.25, 0.25])\n",
    "advantage = rewards - rewards.mean()\n",
    "policy = np.exp(np.log(policy + 1e-6) + 0.5 * advantage)\n",
    "policy /= policy.sum()\n",
    "print('Updated policy distribution:', policy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8a651b",
   "metadata": {},
   "source": [
    "## Try it\n",
    "- Modify the demo\n",
    "- Add a tiny dataset or counter-example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abac7f77",
   "metadata": {},
   "source": [
    "## References\n",
    "- [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
    "- [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)\n",
    "- [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/)\n",
    "- [Demystifying Prompts in Language Models via Perplexity Estimation](https://arxiv.org/abs/2212.04037)\n",
    "- [Calibrate Before Use: Improving Few-Shot Performance of Language Models](https://arxiv.org/abs/2102.09690)\n",
    "- [Holistic Evaluation of Language Models](https://arxiv.org/abs/2211.09110)\n",
    "- [Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?](https://arxiv.org/abs/2202.12837)\n",
    "- [In-context Learning and Induction Heads](https://arxiv.org/abs/2209.11895)\n",
    "- [Multitask Prompted Training Enables Zero-Shot Task Generalization](https://arxiv.org/abs/2110.08207)\n",
    "- [Scaling Instruction-Finetuned Language Models](https://arxiv.org/abs/2210.11416)\n",
    "- [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)\n",
    "- [[Website] Stanford Alpaca: An Instruction-following LLaMA Model](https://crfm.stanford.edu/2023/03/13/alpaca.html)\n",
    "- [Revisiting the Gold Standard: Grounding Summarization Evaluation with Robust Human Evaluation](https://arxiv.org/abs/2212.07981)\n",
    "- [WiCE: Real-World Entailment for Claims in Wikipedia](https://arxiv.org/abs/2303.01432)\n",
    "- [SummaC: Re-Visiting NLI-based Models for Inconsistency Detection in Summarization](https://arxiv.org/abs/2111.09525)\n",
    "- [FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation](https://arxiv.org/abs/2305.14251)\n",
    "- [RARR: Researching and Revising What Language Models Say, Using Language Models](https://arxiv.org/abs/2210.08726)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88c9e24",
   "metadata": {},
   "source": [
    "*Links only; we do not redistribute slides or papers.*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
