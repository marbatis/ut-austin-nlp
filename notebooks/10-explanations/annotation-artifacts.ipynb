{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f9b3787",
   "metadata": {},
   "source": [
    "# Annotation Artifacts\n",
    "\n",
    "- üì∫ **Video:** [https://youtu.be/RXYaMZcDIWU](https://youtu.be/RXYaMZcDIWU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49c9d3d",
   "metadata": {},
   "source": [
    "## Overview\n",
    "Highlights that models often exploit spurious biases in datasets rather than truly solve the task, which we can discover by analyzing errors or using adversarial tests For example, in NLI, the dataset had artifacts like ‚Äúnot‚Äù in hypothesis biases towards contradiction, or certain words like ‚Äúmusical‚Äù in hypothesis almost always entailment - models pick these up. Gururangan et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ec0b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "random.seed(0)\n",
    "CI = os.environ.get('CI') == 'true'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a99bb72",
   "metadata": {},
   "source": [
    "## Key ideas\n",
    "- (2018) found that just the hypothesis alone (without premise) allowed > 50% accuracy on entailment task - meaning dataset wasn't challenging understanding, models can cheat.\n",
    "- Poliak et al.\n",
    "- (2018) ‚ÄúHypothesis only baseline‚Äù further analyzed that for multiple NLI datasets.\n",
    "- This video likely explains that such artifacts arise from how data was collected (e.g., crowdworkers might use some patterns when writing contradictions like starting with ‚ÄúNo‚Äù or ‚ÄúThere is no‚Äù)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235cbbc7",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ea9631",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Try the exercises below and follow the linked materials.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902f5713",
   "metadata": {},
   "source": [
    "## Try it\n",
    "- Modify the demo\n",
    "- Add a tiny dataset or counter-example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c3e785",
   "metadata": {},
   "source": [
    "## References\n",
    "- [The Mythos of Model Interpretability](https://arxiv.org/pdf/1606.03490.pdf)\n",
    "- [Deep Unordered Composition Rivals Syntactic Methods for Text Classification](https://www.aclweb.org/anthology/P15-1162/)\n",
    "- [Analysis Methods in Neural Language Processing: A Survey](https://arxiv.org/pdf/1812.08951.pdf)\n",
    "- [\"Why Should I Trust You?\" Explaining the Predictions of Any Classifier](https://arxiv.org/pdf/1602.04938.pdf)\n",
    "- [Axiomatic Attribution for Deep Networks](https://arxiv.org/pdf/1703.01365.pdf)\n",
    "- [BERT Rediscovers the Classical NLP Pipeline](https://arxiv.org/pdf/1905.05950.pdf)\n",
    "- [What Do You Learn From Context? Probing For Sentence Structure In Contextualized Word Represenations](https://arxiv.org/pdf/1905.06316.pdf)\n",
    "- [Annotation Artifacts in Natural Language Inference Data](https://www.aclweb.org/anthology/N18-2017/)\n",
    "- [Hypothesis Only Baselines in Natural Language Inference](https://www.aclweb.org/anthology/S18-2023/)\n",
    "- [Did the Model Understand the Question?](https://www.aclweb.org/anthology/P18-1176/)\n",
    "- [Swag: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference](https://www.aclweb.org/anthology/D18-1009.pdf)\n",
    "- [Generating Visual Explanations](https://arxiv.org/pdf/1603.08507.pdf)\n",
    "- [e-SNLI: Natural Language Inference with Natural Language Explanations](https://arxiv.org/abs/1812.01193)\n",
    "- [Explaining Question Answering Models through Text Generation](https://arxiv.org/pdf/2004.05569.pdf)\n",
    "- [Program Induction by Rationale Generation : Learning to Solve and Explain Algebraic Word Problems](https://arxiv.org/abs/1705.04146)\n",
    "- [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)\n",
    "- [The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning](https://arxiv.org/abs/2205.03401)\n",
    "- [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/abs/2205.11916)\n",
    "- [Complementary Explanations for Effective In-Context Learning](https://arxiv.org/pdf/2211.13892.pdf)\n",
    "- [PAL: Program-aided Language Models](https://arxiv.org/abs/2211.10435)\n",
    "- [Measuring and Narrowing the Compositionality Gap in Language Models](https://arxiv.org/abs/2210.03350)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430b18fe",
   "metadata": {},
   "source": [
    "*Links only; we do not redistribute slides or papers.*"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
