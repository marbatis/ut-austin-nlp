{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f9b3787",
   "metadata": {},
   "source": [
    "# Annotation Artifacts\n",
    "\n",
    "- ðŸ“º **Video:** [https://youtu.be/RXYaMZcDIWU](https://youtu.be/RXYaMZcDIWU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49c9d3d",
   "metadata": {},
   "source": [
    "## Overview\n",
    "- Identify spurious correlations introduced by dataset annotation shortcuts.\n",
    "- Guard against models exploiting artifacts instead of the intended signal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a99bb72",
   "metadata": {},
   "source": [
    "## Key ideas\n",
    "- **Annotator bias:** crowdworkers use heuristics like lexical cues.\n",
    "- **Artifact detection:** check word frequencies conditional on label.\n",
    "- **Adversarial filtering:** remove or rebalance examples with strong artifacts.\n",
    "- **Evaluation:** compare artifact-heavy vs. artifact-free subsets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235cbbc7",
   "metadata": {},
   "source": [
    "## Demo\n",
    "Compute pointwise mutual information between words and labels on a synthetic dataset to surface artifacts, following the lecture (https://youtu.be/Qtd4QktaSWk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62ea9631",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T01:32:57.950710Z",
     "iopub.status.busy": "2025-10-29T01:32:57.950467Z",
     "iopub.status.idle": "2025-10-29T01:32:57.960788Z",
     "shell.execute_reply": "2025-10-29T01:32:57.960444Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMI(token='no.', label=0) = 0.51\n",
      "PMI(token='answer', label=0) = 0.51\n",
      "PMI(token='evidence', label=0) = 0.51\n",
      "PMI(token='limited,', label=0) = 0.51\n",
      "PMI(token='is', label=0) = 0.11\n",
      "PMI(token='is', label=1) = -0.18\n",
      "PMI(token='the', label=0) = -0.18\n",
      "PMI(token='the', label=1) = 0.22\n",
      "PMI(token='because', label=0) = 0.51\n",
      "PMI(token='multiple', label=1) = 0.92\n",
      "PMI(token='supported', label=1) = 0.92\n",
      "PMI(token='yes,', label=1) = 0.92\n",
      "PMI(token='claim', label=1) = 0.92\n",
      "PMI(token='by', label=1) = 0.92\n",
      "PMI(token='studies.', label=1) = 0.92\n",
      "PMI(token='cannot', label=0) = 0.51\n",
      "PMI(token='it.', label=0) = 0.51\n",
      "PMI(token='witnesses', label=0) = 0.51\n",
      "PMI(token='confirm', label=0) = 0.51\n",
      "PMI(token='disagree,', label=0) = 0.51\n",
      "PMI(token='we', label=0) = 0.51\n",
      "PMI(token='yes.', label=1) = 0.92\n",
      "PMI(token='indicate', label=1) = 0.92\n",
      "PMI(token='results', label=1) = 0.92\n",
      "PMI(token='clearly', label=1) = 0.92\n",
      "PMI(token='missing,', label=0) = 0.51\n",
      "PMI(token='document', label=0) = 0.51\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "examples = [\n",
    "    ('Because evidence is limited, the answer is no.', 0),\n",
    "    ('Yes, the claim is supported by multiple studies.', 1),\n",
    "    ('Because witnesses disagree, we cannot confirm it.', 0),\n",
    "    ('The results clearly indicate yes.', 1),\n",
    "    ('Because the document is missing, answer no.', 0)\n",
    "]\n",
    "\n",
    "word_counts = Counter()\n",
    "label_counts = Counter()\n",
    "joint_counts = Counter()\n",
    "\n",
    "for text, label in examples:\n",
    "    tokens = set(text.lower().split())\n",
    "    label_counts[label] += 1\n",
    "    for token in tokens:\n",
    "        word_counts[token] += 1\n",
    "        joint_counts[(token, label)] += 1\n",
    "\n",
    "for token, count in word_counts.items():\n",
    "    p_token = count / len(examples)\n",
    "    for label in label_counts:\n",
    "        joint = joint_counts[(token, label)] / len(examples)\n",
    "        if joint == 0:\n",
    "            continue\n",
    "        p_label = label_counts[label] / len(examples)\n",
    "        pmi = math.log(joint / (p_token * p_label))\n",
    "        print(f\"PMI(token='{token}', label={label}) = {pmi:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902f5713",
   "metadata": {},
   "source": [
    "## Try it\n",
    "- Modify the demo\n",
    "- Add a tiny dataset or counter-example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c3e785",
   "metadata": {},
   "source": [
    "## References\n",
    "- [The Mythos of Model Interpretability](https://arxiv.org/pdf/1606.03490.pdf)\n",
    "- [Deep Unordered Composition Rivals Syntactic Methods for Text Classification](https://www.aclweb.org/anthology/P15-1162/)\n",
    "- [Analysis Methods in Neural Language Processing: A Survey](https://arxiv.org/pdf/1812.08951.pdf)\n",
    "- [\"Why Should I Trust You?\" Explaining the Predictions of Any Classifier](https://arxiv.org/pdf/1602.04938.pdf)\n",
    "- [Axiomatic Attribution for Deep Networks](https://arxiv.org/pdf/1703.01365.pdf)\n",
    "- [BERT Rediscovers the Classical NLP Pipeline](https://arxiv.org/pdf/1905.05950.pdf)\n",
    "- [What Do You Learn From Context? Probing For Sentence Structure In Contextualized Word Represenations](https://arxiv.org/pdf/1905.06316.pdf)\n",
    "- [Annotation Artifacts in Natural Language Inference Data](https://www.aclweb.org/anthology/N18-2017/)\n",
    "- [Hypothesis Only Baselines in Natural Language Inference](https://www.aclweb.org/anthology/S18-2023/)\n",
    "- [Did the Model Understand the Question?](https://www.aclweb.org/anthology/P18-1176/)\n",
    "- [Swag: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference](https://www.aclweb.org/anthology/D18-1009.pdf)\n",
    "- [Generating Visual Explanations](https://arxiv.org/pdf/1603.08507.pdf)\n",
    "- [e-SNLI: Natural Language Inference with Natural Language Explanations](https://arxiv.org/abs/1812.01193)\n",
    "- [Explaining Question Answering Models through Text Generation](https://arxiv.org/pdf/2004.05569.pdf)\n",
    "- [Program Induction by Rationale Generation : Learning to Solve and Explain Algebraic Word Problems](https://arxiv.org/abs/1705.04146)\n",
    "- [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)\n",
    "- [The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning](https://arxiv.org/abs/2205.03401)\n",
    "- [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/abs/2205.11916)\n",
    "- [Complementary Explanations for Effective In-Context Learning](https://arxiv.org/pdf/2211.13892.pdf)\n",
    "- [PAL: Program-aided Language Models](https://arxiv.org/abs/2211.10435)\n",
    "- [Measuring and Narrowing the Compositionality Gap in Language Models](https://arxiv.org/abs/2210.03350)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430b18fe",
   "metadata": {},
   "source": [
    "*Links only; we do not redistribute slides or papers.*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
