{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9700ee2b",
   "metadata": {},
   "source": [
    "# Chain-of-thought\n",
    "\n",
    "- üì∫ **Video:** [https://youtu.be/tNGu3EqJbKc](https://youtu.be/tNGu3EqJbKc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386fcb6f",
   "metadata": {},
   "source": [
    "## Overview\n",
    "Discusses the approach of prompting models to generate a step-by-step reasoning chain before the final answer This ties with the Wei et al. 2022 paper (Google's ‚ÄúChain-of-thought prompting elicits reasoning‚Äù) which found that if you prompt e.g., ‚ÄúLet's think step by step‚Äù or give examples where the solution is reached via intermediate steps, large LMs (especially with 100B+ params) can solve significantly harder problems (like multi-step math, logical puzzles) compared to giving direct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa9ef27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "random.seed(0)\n",
    "CI = os.environ.get('CI') == 'true'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891ec3fa",
   "metadata": {},
   "source": [
    "## Key ideas\n",
    "- The video likely shows an example: Q: ‚ÄúIf there are 3 apples and you take away 2, how many left?‚Äù A normal prompt might confuse model (some might answer 1, which is correct, but for more complex Q it fails).\n",
    "- With CoT: \"Let's think step by step: Initially 3 apples.\n",
    "- If you take 2, that means 3-2 = 1.\n",
    "- So answer is 1.‚Äù The model outputs the reasoning and final."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939453be",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c51037",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Try the exercises below and follow the linked materials.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f365c167",
   "metadata": {},
   "source": [
    "## Try it\n",
    "- Modify the demo\n",
    "- Add a tiny dataset or counter-example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a21ba0",
   "metadata": {},
   "source": [
    "## References\n",
    "- [The Mythos of Model Interpretability](https://arxiv.org/pdf/1606.03490.pdf)\n",
    "- [Deep Unordered Composition Rivals Syntactic Methods for Text Classification](https://www.aclweb.org/anthology/P15-1162/)\n",
    "- [Analysis Methods in Neural Language Processing: A Survey](https://arxiv.org/pdf/1812.08951.pdf)\n",
    "- [\"Why Should I Trust You?\" Explaining the Predictions of Any Classifier](https://arxiv.org/pdf/1602.04938.pdf)\n",
    "- [Axiomatic Attribution for Deep Networks](https://arxiv.org/pdf/1703.01365.pdf)\n",
    "- [BERT Rediscovers the Classical NLP Pipeline](https://arxiv.org/pdf/1905.05950.pdf)\n",
    "- [What Do You Learn From Context? Probing For Sentence Structure In Contextualized Word Represenations](https://arxiv.org/pdf/1905.06316.pdf)\n",
    "- [Annotation Artifacts in Natural Language Inference Data](https://www.aclweb.org/anthology/N18-2017/)\n",
    "- [Hypothesis Only Baselines in Natural Language Inference](https://www.aclweb.org/anthology/S18-2023/)\n",
    "- [Did the Model Understand the Question?](https://www.aclweb.org/anthology/P18-1176/)\n",
    "- [Swag: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference](https://www.aclweb.org/anthology/D18-1009.pdf)\n",
    "- [Generating Visual Explanations](https://arxiv.org/pdf/1603.08507.pdf)\n",
    "- [e-SNLI: Natural Language Inference with Natural Language Explanations](https://arxiv.org/abs/1812.01193)\n",
    "- [Explaining Question Answering Models through Text Generation](https://arxiv.org/pdf/2004.05569.pdf)\n",
    "- [Program Induction by Rationale Generation : Learning to Solve and Explain Algebraic Word Problems](https://arxiv.org/abs/1705.04146)\n",
    "- [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)\n",
    "- [The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning](https://arxiv.org/abs/2205.03401)\n",
    "- [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/abs/2205.11916)\n",
    "- [Complementary Explanations for Effective In-Context Learning](https://arxiv.org/pdf/2211.13892.pdf)\n",
    "- [PAL: Program-aided Language Models](https://arxiv.org/abs/2211.10435)\n",
    "- [Measuring and Narrowing the Compositionality Gap in Language Models](https://arxiv.org/abs/2210.03350)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3692d50b",
   "metadata": {},
   "source": [
    "*Links only; we do not redistribute slides or papers.*"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
