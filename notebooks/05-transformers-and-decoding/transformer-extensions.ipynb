{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "684bd5b2",
   "metadata": {},
   "source": [
    "# Transformer Extensions\n",
    "\n",
    "- ðŸ“º **Video:** [https://youtu.be/DPvDL8L4Dqo](https://youtu.be/DPvDL8L4Dqo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd0e99f",
   "metadata": {},
   "source": [
    "## Overview\n",
    "- Survey improvements to the vanilla transformer: layer scaling, relative positions, adapters, and sparse attention.\n",
    "- Appreciate why these tweaks help different applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a5b9bc",
   "metadata": {},
   "source": [
    "## Key ideas\n",
    "- **Relative position bias:** improves extrapolation and modeling of long sequences.\n",
    "- **Adapters:** lightweight modules enable parameter-efficient fine-tuning.\n",
    "- **Layer scaling:** stabilize deep stacks via residual scaling or pre-norm.\n",
    "- **Sparse patterns:** reduce quadratic cost for long contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e65c4a5",
   "metadata": {},
   "source": [
    "## Demo\n",
    "Augment a transformer block with a relative-position bias and adapters using PyTorch modules to reflect techniques highlighted in the lecture (https://youtu.be/rZMAM19aP84)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "004000e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T01:29:18.311001Z",
     "iopub.status.busy": "2025-10-29T01:29:18.310714Z",
     "iopub.status.idle": "2025-10-29T01:29:18.987253Z",
     "shell.execute_reply": "2025-10-29T01:29:18.986963Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 5, 16])\n",
      "Attention weights shape: torch.Size([1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Adapter(nn.Module):\n",
    "    def __init__(self, dim, bottleneck=8):\n",
    "        super().__init__()\n",
    "        self.down = nn.Linear(dim, bottleneck)\n",
    "        self.up = nn.Linear(bottleneck, dim)\n",
    "        self.activation = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        return self.up(self.activation(self.down(x)))\n",
    "\n",
    "class RelPosBlock(nn.Module):\n",
    "    def __init__(self, dim, nhead):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(dim, nhead, batch_first=True)\n",
    "        self.ff = nn.Sequential(nn.Linear(dim, 2*dim), nn.ReLU(), nn.Linear(2*dim, dim))\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.adapter = Adapter(dim)\n",
    "        self.rel_bias = nn.Parameter(torch.zeros(nhead, 1, 1))\n",
    "    def forward(self, x):\n",
    "        q = k = v = x\n",
    "        attn_output, weights = self.attn(q, k, v)\n",
    "        attn_output = attn_output + self.rel_bias.mean(dim=0)\n",
    "        x = x + attn_output\n",
    "        x = self.norm1(x)\n",
    "        residual = x\n",
    "        x = self.ff(x) + self.adapter(x)\n",
    "        x = self.norm2(x + residual)\n",
    "        return x, weights\n",
    "\n",
    "block = RelPosBlock(dim=16, nhead=4)\n",
    "inputs = torch.randn(1, 5, 16)\n",
    "output, weights = block(inputs)\n",
    "print('Output shape:', output.shape)\n",
    "print('Attention weights shape:', weights.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e7ef97",
   "metadata": {},
   "source": [
    "## Try it\n",
    "- Modify the demo\n",
    "- Add a tiny dataset or counter-example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99792237",
   "metadata": {},
   "source": [
    "## References\n",
    "- [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "- [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361)\n",
    "- [Efficient Transformers: A Survey](https://arxiv.org/abs/2009.06732)\n",
    "- [Rethinking Attention with Performers](https://arxiv.org/abs/2009.14794)\n",
    "- [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150)\n",
    "- [The Curious Case of Neural Text Degeneration](https://arxiv.org/abs/1904.09751)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805cc95d",
   "metadata": {},
   "source": [
    "*Links only; we do not redistribute slides or papers.*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
