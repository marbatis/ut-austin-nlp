{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c367affd",
   "metadata": {},
   "source": [
    "# Beam Search\n",
    "\n",
    "- ðŸ“º **Video:** [https://youtu.be/wltqDbhlcJ0](https://youtu.be/wltqDbhlcJ0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cb2d40",
   "metadata": {},
   "source": [
    "## Overview\n",
    "- What youâ€™ll learn (fill in after watching)\n",
    "- Why it matters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816af836",
   "metadata": {},
   "source": [
    "## Key ideas\n",
    "- TODO: Summarize the core ideas after viewing the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c370673",
   "metadata": {},
   "source": [
    "## Demo\n",
    "Beam search over a tiny vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c0b1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal beam search over a toy vocabulary\n",
    "vocab = {\n",
    "    \"<s>\": {\"i\": -0.1, \"we\": -0.3},\n",
    "    \"i\": {\"love\": -0.2, \"like\": -0.4},\n",
    "    \"we\": {\"love\": -0.3, \"enjoy\": -0.5},\n",
    "    \"love\": {\"nlp\": -0.1, \"</s>\": -1.0},\n",
    "    \"like\": {\"nlp\": -0.2, \"</s>\": -1.2},\n",
    "    \"enjoy\": {\"transformers\": -0.2, \"</s>\": -1.1},\n",
    "}\n",
    "\n",
    "beam = [(\"<s>\", 0.0)]\n",
    "beam_size = 2\n",
    "\n",
    "for _ in range(3):\n",
    "    candidates = []\n",
    "    for seq, score in beam:\n",
    "        last = seq.split()[-1]\n",
    "        for word, logp in vocab.get(last, {}).items():\n",
    "            candidates.append((seq + \" \" + word, score + logp))\n",
    "    beam = sorted(candidates, key=lambda x: x[1])[:beam_size]\n",
    "\n",
    "for seq, score in beam:\n",
    "    print(f\"{seq}  (logP={score:.2f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eed4f0",
   "metadata": {},
   "source": [
    "## Try it\n",
    "- Modify the demo\n",
    "- Add a tiny dataset or counter-example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c284ea54",
   "metadata": {},
   "source": [
    "## References\n",
    "- [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "- [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361)\n",
    "- [Efficient Transformers: A Survey](https://arxiv.org/abs/2009.06732)\n",
    "- [Rethinking Attention with Performers](https://arxiv.org/abs/2009.14794)\n",
    "- [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150)\n",
    "- [The Curious Case of Neural Text Degeneration](https://arxiv.org/abs/1904.09751)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a98d1be",
   "metadata": {},
   "source": [
    "*Links only; we do not redistribute slides or papers.*"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
