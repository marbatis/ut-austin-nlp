{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17268537",
   "metadata": {},
   "source": [
    "# Nucleus Sampling\n",
    "\n",
    "- ðŸ“º **Video:** [https://youtu.be/JETxaSaj6_k](https://youtu.be/JETxaSaj6_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1688fc7d",
   "metadata": {},
   "source": [
    "## Overview\n",
    "Covers an alternative decoding strategy geared towards more natural and less repetitive text generation: Nucleus Sampling (top-p sampling) The video likely begins by pointing out limitations of deterministic decoding like beam search: for open-ended tasks (like story generation or chat), always taking the highest probability leads to safe, generic, sometimes dull outputs or can get stuck in repetitive loops (a phenomenon called Neural Text Degeneration, as per Holtzman et al. 2019 To produce varied and human-like text, we often use randomness in decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016a4cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "random.seed(0)\n",
    "CI = os.environ.get('CI') == 'true'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d61738d",
   "metadata": {},
   "source": [
    "## Key ideas\n",
    "- Basic random sampling from the full softmax often yields incoherent text because the model's probability tail (many low-probability words) can be sampled, introducing nonsense.\n",
    "- So Top-k sampling was introduced - only sample from the top k most likely words.\n",
    "- Nucleus Sampling further generalizes that: at each step, determine the smallest set of words such that their cumulative probability â‰¥ p (e.g.\n",
    "- p = 0.9), and then sample uniformly from that set according to the model's probabilities This means sometimes the set could be just 1 word (if one word has â‰¥90% probability, we'll deterministically take it), but if the distribution is more uncertain, we might include more options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941f9da8",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41133cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Try the exercises below and follow the linked materials.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9da2e3",
   "metadata": {},
   "source": [
    "## Try it\n",
    "- Modify the demo\n",
    "- Add a tiny dataset or counter-example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3c1b3d",
   "metadata": {},
   "source": [
    "## References\n",
    "- [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "- [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361)\n",
    "- [Efficient Transformers: A Survey](https://arxiv.org/abs/2009.06732)\n",
    "- [Rethinking Attention with Performers](https://arxiv.org/abs/2009.14794)\n",
    "- [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150)\n",
    "- [The Curious Case of Neural Text Degeneration](https://arxiv.org/abs/1904.09751)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d974af9",
   "metadata": {},
   "source": [
    "*Links only; we do not redistribute slides or papers.*"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
