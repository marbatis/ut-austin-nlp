{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17268537",
   "metadata": {},
   "source": [
    "# Nucleus Sampling\n",
    "\n",
    "- ðŸ“º **Video:** [https://youtu.be/JETxaSaj6_k](https://youtu.be/JETxaSaj6_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1688fc7d",
   "metadata": {},
   "source": [
    "## Overview\n",
    "- What youâ€™ll learn (fill in after watching)\n",
    "- Why it matters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d61738d",
   "metadata": {},
   "source": [
    "## Key ideas\n",
    "- TODO: Summarize the core ideas after viewing the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941f9da8",
   "metadata": {},
   "source": [
    "## Demo\n",
    "Nucleus (top-p) sampling on a toy logit distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41133cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nucleus sampling illustration\n",
    "import numpy as np\n",
    "\n",
    "logits = np.array([2.3, 1.2, 0.7, 0.1, -0.5])\n",
    "probs = np.exp(logits - logits.max())\n",
    "probs /= probs.sum()\n",
    "\n",
    "sorted_idx = np.argsort(probs)[::-1]\n",
    "sorted_probs = probs[sorted_idx]\n",
    "cumulative = np.cumsum(sorted_probs)\n",
    "p = 0.9\n",
    "cutoff = np.searchsorted(cumulative, p) + 1\n",
    "\n",
    "support = sorted_idx[:cutoff]\n",
    "top_probs = sorted_probs[:cutoff]\n",
    "top_probs /= top_probs.sum()\n",
    "\n",
    "sample = np.random.choice(support, p=top_probs)\n",
    "print(\"Support after nucleus filter:\", support)\n",
    "print(\"Sampled token id:\", int(sample))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9da2e3",
   "metadata": {},
   "source": [
    "## Try it\n",
    "- Modify the demo\n",
    "- Add a tiny dataset or counter-example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3c1b3d",
   "metadata": {},
   "source": [
    "## References\n",
    "- [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "- [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361)\n",
    "- [Efficient Transformers: A Survey](https://arxiv.org/abs/2009.06732)\n",
    "- [Rethinking Attention with Performers](https://arxiv.org/abs/2009.14794)\n",
    "- [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150)\n",
    "- [The Curious Case of Neural Text Degeneration](https://arxiv.org/abs/1904.09751)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d974af9",
   "metadata": {},
   "source": [
    "*Links only; we do not redistribute slides or papers.*"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
