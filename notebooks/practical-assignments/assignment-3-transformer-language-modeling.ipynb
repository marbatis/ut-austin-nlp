{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d428d9d",
   "metadata": {},
   "source": [
    "# Assignment 3: Transformer Language Modeling\n",
    "\n",
    "- [Prompt](https://www.cs.utexas.edu/~gdurrett/courses/online-course/a3.pdf)\n",
    "- [Starter code + data](https://www.cs.utexas.edu/~gdurrett/courses/online-course/a3-distrib.tgz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d954bca1",
   "metadata": {},
   "source": [
    "## Overview\n",
    "Build and experiment with a Transformer-based language model. The goal is to implement multi-head self-attention, positional encodings, and decoding to generate fluent text, reinforcing Week 5 material."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5984c3",
   "metadata": {},
   "source": [
    "## Key ideas\n",
    "- Implement scaled dot-product attention, multi-head attention, and feed-forward blocks for an autoregressive LM.\n",
    "- Train the model on the provided corpus; track perplexity and sample generations across checkpoints.\n",
    "- Study the effect of architecture/optimisation choices (layers, heads, learning rate schedules).\n",
    "- Analyse generated text for coherence and discuss strengths/limitations compared to n-gram baselines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970bc7b5",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "- Clone/update the assignment starter repository.\n",
    "- Set up the recommended environment (see prompt).\n",
    "- Fill in TODO blocks incrementally, keeping a training log."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49d4737",
   "metadata": {},
   "source": [
    "## Deliverables\n",
    "- Transformer LM training code with reproducible configuration.\n",
    "- Evaluation summary (perplexity, qualitative samples, ablation insights).\n",
    "- Reflection linking results to Week 5 lectures on attention and decoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449ff997",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "- What worked well?\n",
    "- What failed or surprised you?\n",
    "- How does this assignment connect to lecture material?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e88dd438",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T01:15:38.947059Z",
     "iopub.status.busy": "2025-10-29T01:15:38.946482Z",
     "iopub.status.idle": "2025-10-29T01:15:38.955508Z",
     "shell.execute_reply": "2025-10-29T01:15:38.955032Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to prototype!\n"
     ]
    }
   ],
   "source": [
    "# Use this space for quick experiments or sanity checks\n",
    "import random\n",
    "random.seed(0)\n",
    "print('Ready to prototype!')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
