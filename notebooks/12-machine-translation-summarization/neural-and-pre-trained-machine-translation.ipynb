{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e202674",
   "metadata": {},
   "source": [
    "# Neural and Pre-Trained Machine Translation\n",
    "\n",
    "- ðŸ“º **Video:** [https://youtu.be/bcP4b_4HQ8A](https://youtu.be/bcP4b_4HQ8A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f354dac6",
   "metadata": {},
   "source": [
    "## Overview\n",
    "- Review encoder-decoder architectures with attention and modern pre-trained MT models.\n",
    "- Discuss transfer from multilingual pre-training (mBART, mT5)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74ad68d",
   "metadata": {},
   "source": [
    "## Key ideas\n",
    "- **Seq2seq with attention:** decoder attends to encoder states for every target token.\n",
    "- **Pre-training:** multilingual denoising yields strong initialization.\n",
    "- **Fine-tuning:** adapt pre-trained models to domain-specific parallel data.\n",
    "- **Evaluation:** monitor BLEU and human adequacy for domain shifts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e09cd0",
   "metadata": {},
   "source": [
    "## Demo\n",
    "Train a miniature attention-based seq2seq model on synthetic pairs to illustrate attention weights, following the lecture (https://youtu.be/jjaJrCB3Q6c)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a68b5d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T01:34:59.027795Z",
     "iopub.status.busy": "2025-10-29T01:34:59.027416Z",
     "iopub.status.idle": "2025-10-29T01:35:00.603346Z",
     "shell.execute_reply": "2025-10-29T01:35:00.602959Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  50 | loss 1.6629\n",
      "epoch 100 | loss 1.6478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 150 | loss 1.6455\n",
      "epoch 200 | loss 1.6445\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "pairs = [\n",
    "    ('bonjour', 'hello'),\n",
    "    ('salut', 'hi'),\n",
    "    ('merci', 'thanks')\n",
    "]\n",
    "\n",
    "chars = sorted(set(''.join(src for src, _ in pairs) + ''.join(tgt for _, tgt in pairs))) + ['<pad>', '<s>', '</s>']\n",
    "char_to_id = {c: i for i, c in enumerate(chars)}\n",
    "all_texts = [src for src, _ in pairs] + [tgt for _, tgt in pairs]\n",
    "max_len = max(len(text) for text in all_texts) + 2\n",
    "\n",
    "def encode(text):\n",
    "    ids = [char_to_id['<s>']] + [char_to_id[c] for c in text] + [char_to_id['</s>']]\n",
    "    ids += [char_to_id['<pad>']] * (max_len - len(ids))\n",
    "    return ids\n",
    "\n",
    "src = torch.tensor([encode(s) for s, _ in pairs])\n",
    "tgt = torch.tensor([encode(t) for _, t in pairs])\n",
    "\n",
    "embed = nn.Embedding(len(chars), 32)\n",
    "encoder = nn.GRU(32, 32, batch_first=True)\n",
    "decoder = nn.GRU(32, 32, batch_first=True)\n",
    "attn = nn.Linear(32 * 2, 1)\n",
    "out = nn.Linear(32, len(chars))\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=char_to_id['<pad>'])\n",
    "optimizer = torch.optim.Adam(list(embed.parameters()) + list(encoder.parameters()) + list(decoder.parameters()) + list(attn.parameters()) + list(out.parameters()), lr=5e-3)\n",
    "\n",
    "for epoch in range(1, 201):\n",
    "    enc_emb = embed(src)\n",
    "    enc_outputs, hidden = encoder(enc_emb)\n",
    "    dec_emb = embed(tgt[:, :-1])\n",
    "    dec_outputs, _ = decoder(dec_emb, hidden)\n",
    "    contexts = []\n",
    "    for t_step in range(dec_outputs.size(1)):\n",
    "        query = dec_outputs[:, t_step:t_step+1, :].expand_as(enc_outputs)\n",
    "        score = attn(torch.cat([enc_outputs, query], dim=-1))\n",
    "        weights = torch.softmax(score, dim=1)\n",
    "        context = (weights * enc_outputs).sum(dim=1)\n",
    "        contexts.append(context)\n",
    "    contexts = torch.stack(contexts, dim=1)\n",
    "    logits = out(contexts)\n",
    "    loss = criterion(logits.reshape(-1, len(chars)), tgt[:, 1:].reshape(-1))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"epoch {epoch:3d} | loss {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23b4b0a",
   "metadata": {},
   "source": [
    "## Try it\n",
    "- Modify the demo\n",
    "- Add a tiny dataset or counter-example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84128f71",
   "metadata": {},
   "source": [
    "## References\n",
    "- [Eisenstein 18.1](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Eisenstein 18.1](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [HMM-Based Word Alignment in Statistical Translation](https://www.aclweb.org/anthology/C96-2141.pdf)\n",
    "- [Pharaoh: A Beam Search Decoder for Phrase-Based Statistical Machine Translation Models](http://homepages.inf.ed.ac.uk/pkoehn/publications/pharaoh-amta2004.pdf)\n",
    "- [Minimum Error Rate Training in Statistical Machine Translation](https://www.aclweb.org/anthology/P03-1021/)\n",
    "- [Eisenstein 18.4](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Revisiting Low-Resource Neural Machine Translation: A Case Study](https://arxiv.org/abs/1905.11901)\n",
    "- [In Neural Machine Translation, What Does Transfer Learning Transfer?](https://aclanthology.org/2020.acl-main.688/)\n",
    "- [Multilingual Denoising Pre-training for Neural Machine Translation](https://arxiv.org/abs/2001.08210)\n",
    "- [Large Language Models Are State-of-the-Art Evaluators of Translation Quality](https://arxiv.org/abs/2302.14520)\n",
    "- [The use of MMR, diversity-based reranking for reordering documents and producing summaries](https://dl.acm.org/doi/10.1145/290941.291025)\n",
    "- [LexRank: Graph-based Lexical Centrality as Salience in Text Summarization](https://arxiv.org/abs/1109.2128)\n",
    "- [A Scalable Global Model for Summarization](https://www.aclweb.org/anthology/W09-1802/)\n",
    "- [Revisiting the Centroid-based Method: A Strong Baseline for Multi-Document Summarization](https://www.aclweb.org/anthology/W17-4511/)\n",
    "- [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://www.aclweb.org/anthology/2020.acl-main.703/)\n",
    "- [PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization](https://arxiv.org/abs/1912.08777)\n",
    "- [Evaluating Factuality in Generation with Dependency-level Entailment](https://arxiv.org/pdf/2010.05478.pdf)\n",
    "- [Asking and Answering Questions to Evaluate the Factual Consistency of Summaries](https://arxiv.org/abs/2004.04228)\n",
    "- [News Summarization and Evaluation in the Era of GPT-3](https://arxiv.org/abs/2209.12356)\n",
    "- [Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections](https://www.aclweb.org/anthology/P11-1061/)\n",
    "- [Multi-Source Transfer of Delexicalized Dependency Parsers](https://www.aclweb.org/anthology/D11-1006/)\n",
    "- [Massively Multilingual Word Embeddings](https://arxiv.org/pdf/1602.01925.pdf)\n",
    "- [Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond](https://www.aclweb.org/anthology/Q19-1038.pdf)\n",
    "- [How multilingual is Multilingual BERT?](https://www.aclweb.org/anthology/P19-1493.pdf)\n",
    "- [Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data](https://aclanthology.org/2020.acl-main.463/)\n",
    "- [Provable Limitations of Acquiring Meaning from Ungrounded Form: What Will Future Language Models Understand?](https://arxiv.org/abs/2104.10809)\n",
    "- [Entailment Semantics Can Be Extracted from an Ideal Language Model](https://arxiv.org/abs/2209.12407)\n",
    "- [Experience Grounds Language](https://arxiv.org/abs/2004.10151)\n",
    "- [VQA: Visual Question Answering](https://arxiv.org/abs/1505.00468)\n",
    "- [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020)\n",
    "- [The Social Impact of Natural Language Processing](https://aclanthology.org/P16-2096.pdf)\n",
    "- [Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints](https://arxiv.org/pdf/1707.09457.pdf)\n",
    "- [GeoMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models](https://arxiv.org/abs/2205.12247)\n",
    "- [Visually Grounded Reasoning across Languages and Cultures](https://arxiv.org/abs/2109.13238)\n",
    "- [On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?](https://dl.acm.org/doi/10.1145/3442188.3445922)\n",
    "- [RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models](https://arxiv.org/abs/2009.11462)\n",
    "- [Datasheets for Datasets](https://arxiv.org/pdf/1803.09010.pdf)\n",
    "- [Closing the AI Accountability Gap: Defining an End-to-End Framework for Internal Algorithmic Auditing](https://dl.acm.org/doi/pdf/10.1145/3351095.3372873)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f19520",
   "metadata": {},
   "source": [
    "*Links only; we do not redistribute slides or papers.*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
