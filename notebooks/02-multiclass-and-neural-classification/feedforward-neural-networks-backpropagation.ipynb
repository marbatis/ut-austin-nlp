{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27714193",
   "metadata": {},
   "source": [
    "# Feedforward Neural Networks, Backpropagation\n",
    "\n",
    "- ðŸ“º **Video:** [https://youtu.be/8WhPYIWyR5g](https://youtu.be/8WhPYIWyR5g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9a6ee0",
   "metadata": {},
   "source": [
    "## Overview\n",
    "- Review how forward and backward passes compute gradients for multilayer perceptrons.\n",
    "- Link analytical derivatives to practical training loops that update weights by gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212dbcd3",
   "metadata": {},
   "source": [
    "## Key ideas\n",
    "- **Composition:** activations compose linear layers with nonlinearities to create expressive decision boundaries.\n",
    "- **Chain rule:** backpropagation reuses partial derivatives layer by layer, reducing computation dramatically.\n",
    "- **Loss gradients:** gradients with respect to weights and biases drive learning.\n",
    "- **Implementation:** vectorized NumPy code mirrors the equations derived in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03eb4f0",
   "metadata": {},
   "source": [
    "## Demo\n",
    "Implement a two-layer neural network from scratch on a moons dataset, explicitly coding the backward pass as shown in the lecture (https://youtu.be/pTWVHNJImDM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa7ea6b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T01:26:08.751185Z",
     "iopub.status.busy": "2025-10-29T01:26:08.750925Z",
     "iopub.status.idle": "2025-10-29T01:26:09.536619Z",
     "shell.execute_reply": "2025-10-29T01:26:09.536332Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100 | loss 0.2307 | train acc 0.898\n",
      "epoch 200 | loss 0.1876 | train acc 0.914\n",
      "epoch 300 | loss 0.1265 | train acc 0.960\n",
      "epoch 400 | loss 0.0899 | train acc 0.967\n",
      "\n",
      "Test accuracy: 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "X, y = make_moons(n_samples=600, noise=0.2, random_state=0)\n",
    "y_onehot = np.eye(2)[y]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.3, random_state=0)\n",
    "\n",
    "input_dim = X.shape[1]\n",
    "hidden_dim = 16\n",
    "output_dim = 2\n",
    "\n",
    "W1 = rng.normal(scale=0.3, size=(input_dim, hidden_dim))\n",
    "B1 = np.zeros(hidden_dim)\n",
    "W2 = rng.normal(scale=0.3, size=(hidden_dim, output_dim))\n",
    "B2 = np.zeros(output_dim)\n",
    "\n",
    "learning_rate = 0.5\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "for epoch in range(1, 401):\n",
    "    z1 = X_train @ W1 + B1\n",
    "    a1 = relu(z1)\n",
    "    logits = a1 @ W2 + B2\n",
    "    exp = np.exp(logits - logits.max(axis=1, keepdims=True))\n",
    "    probs = exp / exp.sum(axis=1, keepdims=True)\n",
    "\n",
    "    loss = -np.mean(np.sum(y_train * np.log(probs + 1e-8), axis=1))\n",
    "\n",
    "    grad_logits = (probs - y_train) / len(X_train)\n",
    "    grad_W2 = a1.T @ grad_logits\n",
    "    grad_B2 = grad_logits.sum(axis=0)\n",
    "\n",
    "    grad_a1 = grad_logits @ W2.T\n",
    "    grad_z1 = grad_a1 * (z1 > 0)\n",
    "    grad_W1 = X_train.T @ grad_z1\n",
    "    grad_B1 = grad_z1.sum(axis=0)\n",
    "\n",
    "    W2 -= learning_rate * grad_W2\n",
    "    B2 -= learning_rate * grad_B2\n",
    "    W1 -= learning_rate * grad_W1\n",
    "    B1 -= learning_rate * grad_B1\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        preds = np.argmax(probs, axis=1)\n",
    "        acc = accuracy_score(np.argmax(y_train, axis=1), preds)\n",
    "        print(f\"epoch {epoch:3d} | loss {loss:.4f} | train acc {acc:.3f}\")\n",
    "\n",
    "z1_test = X_test @ W1 + B1\n",
    "probs_test = np.exp(relu(z1_test) @ W2 + B2)\n",
    "probs_test /= probs_test.sum(axis=1, keepdims=True)\n",
    "\n",
    "test_acc = accuracy_score(np.argmax(y_test, axis=1), np.argmax(probs_test, axis=1))\n",
    "print()\n",
    "print('Test accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be8d726",
   "metadata": {},
   "source": [
    "## Try it\n",
    "- Modify the demo\n",
    "- Add a tiny dataset or counter-example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca5ba85",
   "metadata": {},
   "source": [
    "## References\n",
    "- [Eisenstein 4.2](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Multiclass lecture note](https://www.cs.utexas.edu/~gdurrett/courses/online-course/multiclass.pdf)\n",
    "- [A large annotated corpus for learning natural language inference](https://www.aclweb.org/anthology/D15-1075/)\n",
    "- [Authorship Attribution of Micro-Messages](https://www.aclweb.org/anthology/D13-1193/)\n",
    "- [50 Years of Test (Un)fairness: Lessons for Machine Learning](https://arxiv.org/pdf/1811.10104.pdf)\n",
    "- [[Article] Amazon scraps secret AI recruiting tool that showed bias against women](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G)\n",
    "- [[Blog] Neural Networks, Manifolds, and Topology](http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/)\n",
    "- [Eisenstein Chapter 3.1-3.3](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Dropout: a simple way to prevent neural networks from overfitting](https://dl.acm.org/doi/10.5555/2627435.2670313)\n",
    "- [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)\n",
    "- [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980)\n",
    "- [The Marginal Value of Adaptive Gradient Methods in Machine Learning](https://papers.nips.cc/paper/2017/hash/81b3833e2504647f9d794f7d7b9bf341-Abstract.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa8d19d",
   "metadata": {},
   "source": [
    "*Links only; we do not redistribute slides or papers.*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
