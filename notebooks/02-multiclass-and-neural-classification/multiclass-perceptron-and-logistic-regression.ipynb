{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f0542ff",
   "metadata": {},
   "source": [
    "# Multiclass Perceptron and Logistic Regression\n",
    "\n",
    "- ðŸ“º **Video:** [https://youtu.be/EA627DC7k6M](https://youtu.be/EA627DC7k6M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0258992",
   "metadata": {},
   "source": [
    "## Overview\n",
    "- See how the multiclass perceptron extends the binary mistake-driven update rule to linearly separable problems with more than two labels.\n",
    "- Understand why multinomial logistic regression provides probabilistic scores and smoother updates that converge even on data that is not strictly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4080b472",
   "metadata": {},
   "source": [
    "## Key ideas\n",
    "- **One-vs-all perceptron:** maintain a weight vector per class and update only the true and predicted classes on each mistake.\n",
    "- **Multinomial logistic regression:** optimize a softmax cross-entropy loss with gradient-based updates, yielding calibrated probabilities.\n",
    "- **Regularisation and convergence:** logistic regression benefits from L2 penalties and convexity, giving stable solutions even with noisy features.\n",
    "- **Evaluation:** inspect accuracy and per-class precision/recall to see how the two models behave on overlapping class clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7916614c",
   "metadata": {},
   "source": [
    "## Demo\n",
    "Train a multiclass perceptron and a multinomial logistic regression model on noisy blobs to compare accuracy and per-class precision/recall.\n",
    "The lecture video (https://youtu.be/EA627DC7k6M) motivates these algorithms; this demo shows them in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2b9779d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T01:26:13.251760Z",
     "iopub.status.busy": "2025-10-29T01:26:13.251487Z",
     "iopub.status.idle": "2025-10-29T01:26:14.080089Z",
     "shell.execute_reply": "2025-10-29T01:26:14.079821Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiclass Perceptron\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.769     1.000     0.870        70\n",
      "           1      1.000     0.700     0.824        70\n",
      "           2      1.000     1.000     1.000        70\n",
      "\n",
      "    accuracy                          0.900       210\n",
      "   macro avg      0.923     0.900     0.898       210\n",
      "weighted avg      0.923     0.900     0.898       210\n",
      "\n",
      "\n",
      "Multinomial Logistic Regression\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.956     0.929     0.942        70\n",
      "           1      0.931     0.957     0.944        70\n",
      "           2      1.000     1.000     1.000        70\n",
      "\n",
      "    accuracy                          0.962       210\n",
      "   macro avg      0.962     0.962     0.962       210\n",
      "weighted avg      0.962     0.962     0.962       210\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcelosilveira/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.linear_model import Perceptron, LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_blobs(n_samples=600, centers=3, cluster_std=2.2, random_state=7)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35, random_state=7, stratify=y)\n",
    "\n",
    "perceptron = Perceptron(max_iter=1000, tol=1e-3, random_state=7)\n",
    "logistic = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='lbfgs', random_state=7)\n",
    "\n",
    "perceptron.fit(X_train, y_train)\n",
    "logistic.fit(X_train, y_train)\n",
    "\n",
    "y_pred_perc = perceptron.predict(X_test)\n",
    "y_pred_log = logistic.predict(X_test)\n",
    "\n",
    "print('Multiclass Perceptron')\n",
    "\n",
    "print()\n",
    "print(classification_report(y_test, y_pred_perc, digits=3))\n",
    "print()\n",
    "print('Multinomial Logistic Regression')\n",
    "print()\n",
    "print(classification_report(y_test, y_pred_log, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d9c5ed",
   "metadata": {},
   "source": [
    "## Try it\n",
    "- Modify the demo\n",
    "- Add a tiny dataset or counter-example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898d0cde",
   "metadata": {},
   "source": [
    "## References\n",
    "- [Eisenstein 4.2](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Multiclass lecture note](https://www.cs.utexas.edu/~gdurrett/courses/online-course/multiclass.pdf)\n",
    "- [A large annotated corpus for learning natural language inference](https://www.aclweb.org/anthology/D15-1075/)\n",
    "- [Authorship Attribution of Micro-Messages](https://www.aclweb.org/anthology/D13-1193/)\n",
    "- [50 Years of Test (Un)fairness: Lessons for Machine Learning](https://arxiv.org/pdf/1811.10104.pdf)\n",
    "- [[Article] Amazon scraps secret AI recruiting tool that showed bias against women](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G)\n",
    "- [[Blog] Neural Networks, Manifolds, and Topology](http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/)\n",
    "- [Eisenstein Chapter 3.1-3.3](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Dropout: a simple way to prevent neural networks from overfitting](https://dl.acm.org/doi/10.5555/2627435.2670313)\n",
    "- [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)\n",
    "- [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980)\n",
    "- [The Marginal Value of Adaptive Gradient Methods in Machine Learning](https://papers.nips.cc/paper/2017/hash/81b3833e2504647f9d794f7d7b9bf341-Abstract.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37f12df",
   "metadata": {},
   "source": [
    "*Links only; we do not redistribute slides or papers.*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
