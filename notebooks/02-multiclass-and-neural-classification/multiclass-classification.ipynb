{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3305afc9",
   "metadata": {},
   "source": [
    "# Multiclass Classification\n",
    "\n",
    "- ðŸ“º **Video:** [https://youtu.be/My6GaGhqxdI](https://youtu.be/My6GaGhqxdI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b171b87c",
   "metadata": {},
   "source": [
    "## Overview\n",
    "- Extend binary linear models to multiclass problems using one-vs-rest, one-vs-one, and softmax formulations.\n",
    "- Compare the modeling and computational trade-offs between these strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addfbbf4",
   "metadata": {},
   "source": [
    "## Key ideas\n",
    "- **One-vs-rest:** fit K binary classifiers and choose the argmax score.\n",
    "- **One-vs-one:** train pairwise classifiers when classes are numerous but data per pair is small.\n",
    "- **Softmax regression:** a single model shares features and normalizes scores into probabilities.\n",
    "- **Evaluation:** macro-averaged metrics emphasize performance on minority classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0671e45e",
   "metadata": {},
   "source": [
    "## Demo\n",
    "Contrast one-vs-rest and multinomial logistic regression on the same dataset to show their output distributions, as discussed in the lecture (https://youtu.be/SbgP-3NBp58)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b211e2fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T01:26:11.745116Z",
     "iopub.status.busy": "2025-10-29T01:26:11.744729Z",
     "iopub.status.idle": "2025-10-29T01:26:12.598085Z",
     "shell.execute_reply": "2025-10-29T01:26:12.597782Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-vs-rest LinearSVC\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.854     0.933     0.892       150\n",
      "           1      0.667     0.787     0.722       150\n",
      "           2      0.740     0.740     0.740       150\n",
      "           3      0.661     0.480     0.556       150\n",
      "\n",
      "    accuracy                          0.735       600\n",
      "   macro avg      0.730     0.735     0.727       600\n",
      "weighted avg      0.730     0.735     0.727       600\n",
      "\n",
      "\n",
      "Multinomial logistic regression\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.907     0.913     0.910       150\n",
      "           1      0.732     0.727     0.729       150\n",
      "           2      0.789     0.800     0.795       150\n",
      "           3      0.669     0.660     0.664       150\n",
      "\n",
      "    accuracy                          0.775       600\n",
      "   macro avg      0.774     0.775     0.775       600\n",
      "weighted avg      0.774     0.775     0.775       600\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcelosilveira/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X, y = make_blobs(n_samples=600, centers=4, cluster_std=2.3, random_state=9)\n",
    "\n",
    "ovr = OneVsRestClassifier(LinearSVC(max_iter=2000, dual=False))\n",
    "ovr.fit(X, y)\n",
    "softmax = LogisticRegression(max_iter=2000, multi_class='multinomial', solver='lbfgs')\n",
    "softmax.fit(X, y)\n",
    "\n",
    "print('One-vs-rest LinearSVC')\n",
    "\n",
    "print()\n",
    "print(classification_report(y, ovr.predict(X), digits=3))\n",
    "print()\n",
    "print('Multinomial logistic regression')\n",
    "print()\n",
    "print(classification_report(y, softmax.predict(X), digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9187f023",
   "metadata": {},
   "source": [
    "## Try it\n",
    "- Modify the demo\n",
    "- Add a tiny dataset or counter-example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d7fee7",
   "metadata": {},
   "source": [
    "## References\n",
    "- [Eisenstein 4.2](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Multiclass lecture note](https://www.cs.utexas.edu/~gdurrett/courses/online-course/multiclass.pdf)\n",
    "- [A large annotated corpus for learning natural language inference](https://www.aclweb.org/anthology/D15-1075/)\n",
    "- [Authorship Attribution of Micro-Messages](https://www.aclweb.org/anthology/D13-1193/)\n",
    "- [50 Years of Test (Un)fairness: Lessons for Machine Learning](https://arxiv.org/pdf/1811.10104.pdf)\n",
    "- [[Article] Amazon scraps secret AI recruiting tool that showed bias against women](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G)\n",
    "- [[Blog] Neural Networks, Manifolds, and Topology](http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/)\n",
    "- [Eisenstein Chapter 3.1-3.3](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Dropout: a simple way to prevent neural networks from overfitting](https://dl.acm.org/doi/10.5555/2627435.2670313)\n",
    "- [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)\n",
    "- [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980)\n",
    "- [The Marginal Value of Adaptive Gradient Methods in Machine Learning](https://papers.nips.cc/paper/2017/hash/81b3833e2504647f9d794f7d7b9bf341-Abstract.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa96fb86",
   "metadata": {},
   "source": [
    "*Links only; we do not redistribute slides or papers.*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
