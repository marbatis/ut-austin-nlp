{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28a81aed",
   "metadata": {},
   "source": [
    "# Neural Network Visualization\n",
    "\n",
    "- ðŸ“º **Video:** [https://youtu.be/rdohzaGa8aE](https://youtu.be/rdohzaGa8aE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe18cd6",
   "metadata": {},
   "source": [
    "## Overview\n",
    "- Interpret hidden representations by probing activations and decision boundaries.\n",
    "- Use visualization to debug saturated neurons or dead ReLU units."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665fa46c",
   "metadata": {},
   "source": [
    "## Key ideas\n",
    "- **Activation patterns:** inspect hidden-layer outputs to understand feature learning.\n",
    "- **Saliency:** gradients highlight input dimensions that drive predictions.\n",
    "- **Dimensionality reduction:** project hidden states with PCA/TSNE for qualitative analysis.\n",
    "- **Diagnostics:** spotting dead neurons or exploding activations informs architecture tweaks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc403164",
   "metadata": {},
   "source": [
    "## Demo\n",
    "Train a small MLP, collect hidden activations, and compute a simple saliency map to mirror the lecture (https://youtu.be/v44sQ0IpVDs) on visualization strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e246fca7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T01:26:18.151278Z",
     "iopub.status.busy": "2025-10-29T01:26:18.151139Z",
     "iopub.status.idle": "2025-10-29T01:26:19.057295Z",
     "shell.execute_reply": "2025-10-29T01:26:19.057004Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100 | loss 0.3270 | train acc 0.854\n",
      "epoch 200 | loss 0.3057 | train acc 0.850\n",
      "epoch 300 | loss 0.2969 | train acc 0.861\n",
      "\n",
      "First two PCA components of hidden states (first 5 rows):\n",
      "[[ 1.53146837 -0.839845  ]\n",
      " [-1.26497468  0.39839427]\n",
      " [-1.11865191  0.23210566]\n",
      " [ 1.05886699  0.46724442]\n",
      " [ 1.70560633  0.13048269]]\n",
      "\n",
      "Saliency scores for first test point: [-0.10720718  0.11882685]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "rng = np.random.default_rng(5)\n",
    "X, y = make_moons(n_samples=400, noise=0.25, random_state=5)\n",
    "y_onehot = np.eye(2)[y]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.3, random_state=5)\n",
    "\n",
    "input_dim, hidden_dim, output_dim = 2, 10, 2\n",
    "W1 = rng.normal(scale=0.4, size=(input_dim, hidden_dim))\n",
    "B1 = np.zeros(hidden_dim)\n",
    "W2 = rng.normal(scale=0.4, size=(hidden_dim, output_dim))\n",
    "B2 = np.zeros(output_dim)\n",
    "\n",
    "lr = 0.2\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "for epoch in range(1, 301):\n",
    "    z1 = X_train @ W1 + B1\n",
    "    a1 = relu(z1)\n",
    "    logits = a1 @ W2 + B2\n",
    "    exp = np.exp(logits - logits.max(axis=1, keepdims=True))\n",
    "    probs = exp / exp.sum(axis=1, keepdims=True)\n",
    "    loss = -np.mean(np.sum(y_train * np.log(probs + 1e-8), axis=1))\n",
    "\n",
    "    grad_logits = (probs - y_train) / len(X_train)\n",
    "    grad_W2 = a1.T @ grad_logits\n",
    "    grad_B2 = grad_logits.sum(axis=0)\n",
    "    grad_a1 = grad_logits @ W2.T\n",
    "    grad_z1 = grad_a1 * (z1 > 0)\n",
    "    grad_W1 = X_train.T @ grad_z1\n",
    "    grad_B1 = grad_z1.sum(axis=0)\n",
    "\n",
    "    W2 -= lr * grad_W2\n",
    "    B2 -= lr * grad_B2\n",
    "    W1 -= lr * grad_W1\n",
    "    B1 -= lr * grad_B1\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        preds = np.argmax(probs, axis=1)\n",
    "        acc = accuracy_score(np.argmax(y_train, axis=1), preds)\n",
    "        print(f\"epoch {epoch:3d} | loss {loss:.4f} | train acc {acc:.3f}\")\n",
    "\n",
    "hidden_train = relu(X_train @ W1 + B1)\n",
    "pca = PCA(n_components=2)\n",
    "proj = pca.fit_transform(hidden_train)\n",
    "print()\n",
    "print('First two PCA components of hidden states (first 5 rows):')\n",
    "print(proj[:5])\n",
    "\n",
    "sample = X_test[0]\n",
    "sample_label = np.argmax(y_test[0])\n",
    "perturb = np.zeros_like(sample)\n",
    "eps = 1e-3\n",
    "\n",
    "for i in range(len(sample)):\n",
    "    pert = sample.copy()\n",
    "    pert[i] += eps\n",
    "    z1_pos = pert @ W1 + B1\n",
    "    logits_pos = relu(z1_pos) @ W2 + B2\n",
    "    prob_pos = np.exp(logits_pos - logits_pos.max())\n",
    "    prob_pos /= prob_pos.sum()\n",
    "\n",
    "    z1_neg = sample.copy()\n",
    "    z1_neg[i] -= eps\n",
    "    z1n = z1_neg @ W1 + B1\n",
    "    logits_neg = relu(z1n) @ W2 + B2\n",
    "    prob_neg = np.exp(logits_neg - logits_neg.max())\n",
    "    prob_neg /= prob_neg.sum()\n",
    "\n",
    "    perturb[i] = (prob_pos[sample_label] - prob_neg[sample_label]) / (2 * eps)\n",
    "\n",
    "print()\n",
    "print('Saliency scores for first test point:', perturb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f860313",
   "metadata": {},
   "source": [
    "## Try it\n",
    "- Modify the demo\n",
    "- Add a tiny dataset or counter-example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977b499c",
   "metadata": {},
   "source": [
    "## References\n",
    "- [Eisenstein 4.2](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Multiclass lecture note](https://www.cs.utexas.edu/~gdurrett/courses/online-course/multiclass.pdf)\n",
    "- [A large annotated corpus for learning natural language inference](https://www.aclweb.org/anthology/D15-1075/)\n",
    "- [Authorship Attribution of Micro-Messages](https://www.aclweb.org/anthology/D13-1193/)\n",
    "- [50 Years of Test (Un)fairness: Lessons for Machine Learning](https://arxiv.org/pdf/1811.10104.pdf)\n",
    "- [[Article] Amazon scraps secret AI recruiting tool that showed bias against women](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G)\n",
    "- [[Blog] Neural Networks, Manifolds, and Topology](http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/)\n",
    "- [Eisenstein Chapter 3.1-3.3](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Dropout: a simple way to prevent neural networks from overfitting](https://dl.acm.org/doi/10.5555/2627435.2670313)\n",
    "- [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)\n",
    "- [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980)\n",
    "- [The Marginal Value of Adaptive Gradient Methods in Machine Learning](https://papers.nips.cc/paper/2017/hash/81b3833e2504647f9d794f7d7b9bf341-Abstract.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a14a8fb",
   "metadata": {},
   "source": [
    "*Links only; we do not redistribute slides or papers.*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
