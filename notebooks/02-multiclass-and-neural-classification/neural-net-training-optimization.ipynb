{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "867ce7d4",
   "metadata": {},
   "source": [
    "# Neural Net Training, Optimization\n",
    "\n",
    "- ðŸ“º **Video:** [https://youtu.be/KPZb2rYS4BE](https://youtu.be/KPZb2rYS4BE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae199e6",
   "metadata": {},
   "source": [
    "## Overview\n",
    "- Investigate how optimizer choice and hyperparameters affect neural network training speed and stability.\n",
    "- Diagnose underfitting versus overfitting through learning curves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8a7656",
   "metadata": {},
   "source": [
    "## Key ideas\n",
    "- **Learning rate:** too large causes divergence; too small slows progress.\n",
    "- **Momentum/Adam:** adaptive optimizers smooth noisy gradients.\n",
    "- **Regularization:** dropout or weight decay prevent overfitting.\n",
    "- **Monitoring:** track validation loss to time early stopping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39589ad",
   "metadata": {},
   "source": [
    "## Demo\n",
    "Train the same MLP with plain SGD and with Adam-style momentum on a noisy dataset and compare their loss trajectories, echoing the lecture (https://youtu.be/aQNySJU0vZ4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fab6a995",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T01:26:16.514969Z",
     "iopub.status.busy": "2025-10-29T01:26:16.514690Z",
     "iopub.status.idle": "2025-10-29T01:26:17.403820Z",
     "shell.execute_reply": "2025-10-29T01:26:17.403518Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100 | SGD acc 0.890 | Adam-style acc 0.946\n",
      "epoch 200 | SGD acc 0.892 | Adam-style acc 0.950\n",
      "epoch 300 | SGD acc 0.898 | Adam-style acc 0.950\n",
      "epoch 400 | SGD acc 0.896 | Adam-style acc 0.950\n",
      "Final accuracy (sgd): 0.896\n",
      "Final accuracy (adam): 0.95\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "rng = np.random.default_rng(7)\n",
    "X, y = make_moons(n_samples=500, noise=0.25, random_state=7)\n",
    "y_onehot = np.eye(2)[y]\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "input_dim = X.shape[1]\n",
    "hidden = 20\n",
    "output_dim = 2\n",
    "\n",
    "weights = {\n",
    "    'sgd': {\n",
    "        'W1': rng.normal(scale=0.3, size=(input_dim, hidden)),\n",
    "        'B1': np.zeros(hidden),\n",
    "        'W2': rng.normal(scale=0.3, size=(hidden, output_dim)),\n",
    "        'B2': np.zeros(output_dim)\n",
    "    },\n",
    "    'adam': {\n",
    "        'W1': rng.normal(scale=0.3, size=(input_dim, hidden)),\n",
    "        'B1': np.zeros(hidden),\n",
    "        'W2': rng.normal(scale=0.3, size=(hidden, output_dim)),\n",
    "        'B2': np.zeros(output_dim)\n",
    "    }\n",
    "}\n",
    "\n",
    "adam_m = {key: {k: np.zeros_like(v) for k, v in params.items()} for key, params in weights.items()}\n",
    "adam_v = {key: {k: np.zeros_like(v) for k, v in params.items()} for key, params in weights.items()}\n",
    "\n",
    "sgd_lr = 0.3\n",
    "adam_lr = 0.05\n",
    "beta1, beta2 = 0.9, 0.999\n",
    "\n",
    "for epoch in range(1, 401):\n",
    "    for mode in ['sgd', 'adam']:\n",
    "        W1, B1, W2, B2 = weights[mode]['W1'], weights[mode]['B1'], weights[mode]['W2'], weights[mode]['B2']\n",
    "        z1 = X @ W1 + B1\n",
    "        a1 = relu(z1)\n",
    "        logits = a1 @ W2 + B2\n",
    "        exp = np.exp(logits - logits.max(axis=1, keepdims=True))\n",
    "        probs = exp / exp.sum(axis=1, keepdims=True)\n",
    "        loss = -np.mean(np.sum(y_onehot * np.log(probs + 1e-8), axis=1))\n",
    "\n",
    "        grad_logits = (probs - y_onehot) / len(X)\n",
    "        grad_W2 = a1.T @ grad_logits\n",
    "        grad_B2 = grad_logits.sum(axis=0)\n",
    "        grad_a1 = grad_logits @ W2.T\n",
    "        grad_z1 = grad_a1 * (z1 > 0)\n",
    "        grad_W1 = X.T @ grad_z1\n",
    "        grad_B1 = grad_z1.sum(axis=0)\n",
    "\n",
    "        if mode == 'sgd':\n",
    "            W2 -= sgd_lr * grad_W2\n",
    "            B2 -= sgd_lr * grad_B2\n",
    "            W1 -= sgd_lr * grad_W1\n",
    "            B1 -= sgd_lr * grad_B1\n",
    "        else:\n",
    "            for name, grad in [('W2', grad_W2), ('B2', grad_B2), ('W1', grad_W1), ('B1', grad_B1)]:\n",
    "                adam_m[mode][name] = beta1 * adam_m[mode][name] + (1 - beta1) * grad\n",
    "                adam_v[mode][name] = beta2 * adam_v[mode][name] + (1 - beta2) * (grad ** 2)\n",
    "                m_hat = adam_m[mode][name] / (1 - beta1 ** epoch)\n",
    "                v_hat = adam_v[mode][name] / (1 - beta2 ** epoch)\n",
    "                weights[mode][name] -= adam_lr * m_hat / (np.sqrt(v_hat) + 1e-8)\n",
    "\n",
    "        weights[mode]['W1'], weights[mode]['B1'], weights[mode]['W2'], weights[mode]['B2'] = W1, B1, W2, B2\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        metrics = {}\n",
    "        for mode in ['sgd', 'adam']:\n",
    "            W1, B1, W2, B2 = weights[mode]['W1'], weights[mode]['B1'], weights[mode]['W2'], weights[mode]['B2']\n",
    "            logits = relu(X @ W1 + B1) @ W2 + B2\n",
    "            probs = np.exp(logits - logits.max(axis=1, keepdims=True))\n",
    "            probs /= probs.sum(axis=1, keepdims=True)\n",
    "            preds = np.argmax(probs, axis=1)\n",
    "            metrics[mode] = accuracy_score(y, preds)\n",
    "        print(f\"epoch {epoch:3d} | SGD acc {metrics['sgd']:.3f} | Adam-style acc {metrics['adam']:.3f}\")\n",
    "\n",
    "for mode in ['sgd', 'adam']:\n",
    "    W1, B1, W2, B2 = weights[mode]['W1'], weights[mode]['B1'], weights[mode]['W2'], weights[mode]['B2']\n",
    "    logits = relu(X @ W1 + B1) @ W2 + B2\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    print(f\"Final accuracy ({mode}):\", accuracy_score(y, preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2d663e",
   "metadata": {},
   "source": [
    "## Try it\n",
    "- Modify the demo\n",
    "- Add a tiny dataset or counter-example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77a536e",
   "metadata": {},
   "source": [
    "## References\n",
    "- [Eisenstein 4.2](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Multiclass lecture note](https://www.cs.utexas.edu/~gdurrett/courses/online-course/multiclass.pdf)\n",
    "- [A large annotated corpus for learning natural language inference](https://www.aclweb.org/anthology/D15-1075/)\n",
    "- [Authorship Attribution of Micro-Messages](https://www.aclweb.org/anthology/D13-1193/)\n",
    "- [50 Years of Test (Un)fairness: Lessons for Machine Learning](https://arxiv.org/pdf/1811.10104.pdf)\n",
    "- [[Article] Amazon scraps secret AI recruiting tool that showed bias against women](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G)\n",
    "- [[Blog] Neural Networks, Manifolds, and Topology](http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/)\n",
    "- [Eisenstein Chapter 3.1-3.3](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Dropout: a simple way to prevent neural networks from overfitting](https://dl.acm.org/doi/10.5555/2627435.2670313)\n",
    "- [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)\n",
    "- [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980)\n",
    "- [The Marginal Value of Adaptive Gradient Methods in Machine Learning](https://papers.nips.cc/paper/2017/hash/81b3833e2504647f9d794f7d7b9bf341-Abstract.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a2a304",
   "metadata": {},
   "source": [
    "*Links only; we do not redistribute slides or papers.*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
