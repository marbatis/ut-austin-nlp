{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1374d48",
   "metadata": {},
   "source": [
    "# Fairness in Classification\n",
    "\n",
    "- üì∫ **Video:** [https://youtu.be/N4f2-S19LME](https://youtu.be/N4f2-S19LME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f66db77",
   "metadata": {},
   "source": [
    "## Overview\n",
    "An important interlude addressing the ethical aspect of classification models, specifically focusing on fairness and bias. This video likely discusses how NLP classifiers (and ML models in general) can inadvertently become biased against certain groups if the training data reflect societal biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d65c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "random.seed(0)\n",
    "CI = os.environ.get('CI') == 'true'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f943073",
   "metadata": {},
   "source": [
    "## Key ideas\n",
    "- It introduces the concept of fairness criteria - for example, requiring equal accuracy for different demographic groups or equal false positive rates, etc.\n",
    "- The lecture references a seminal work by Hutchinson & Mitchell (2018) which surveys 50 years of fairness research emphasizing that concerns about biased decision-making long predate modern ML and many definitions of fairness exist (no single metric captures everything).\n",
    "- The video may provide concrete NLP examples: one famous case is an AI recruiting tool at Amazon that was found to be biased against women Specifically, that tool learned from historical hiring data (mostly male) and started down-ranking resumes that contained the word ‚Äúwomen's‚Äù (as in ‚Äúwomen's chess club‚Äù) or that came from women's colleges.\n",
    "- By mentioning this example, the video highlights how a seemingly innocuous model can perpetuate historical discrimination if not carefully designed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278f4855",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bc4ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Try the exercises below and follow the linked materials.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f950908f",
   "metadata": {},
   "source": [
    "## Try it\n",
    "- Modify the demo\n",
    "- Add a tiny dataset or counter-example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e986c8",
   "metadata": {},
   "source": [
    "## References\n",
    "- [Eisenstein 4.2](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Multiclass lecture note](https://www.cs.utexas.edu/~gdurrett/courses/online-course/multiclass.pdf)\n",
    "- [A large annotated corpus for learning natural language inference](https://www.aclweb.org/anthology/D15-1075/)\n",
    "- [Authorship Attribution of Micro-Messages](https://www.aclweb.org/anthology/D13-1193/)\n",
    "- [50 Years of Test (Un)fairness: Lessons for Machine Learning](https://arxiv.org/pdf/1811.10104.pdf)\n",
    "- [[Article] Amazon scraps secret AI recruiting tool that showed bias against women](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G)\n",
    "- [[Blog] Neural Networks, Manifolds, and Topology](http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/)\n",
    "- [Eisenstein Chapter 3.1-3.3](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Dropout: a simple way to prevent neural networks from overfitting](https://dl.acm.org/doi/10.5555/2627435.2670313)\n",
    "- [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)\n",
    "- [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980)\n",
    "- [The Marginal Value of Adaptive Gradient Methods in Machine Learning](https://papers.nips.cc/paper/2017/hash/81b3833e2504647f9d794f7d7b9bf341-Abstract.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ea4921",
   "metadata": {},
   "source": [
    "*Links only; we do not redistribute slides or papers.*"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
