{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "477d7a8b",
   "metadata": {},
   "source": [
    "# Neural Net Implementation\n",
    "\n",
    "- ðŸ“º **Video:** [https://youtu.be/IRZCQO18QAI](https://youtu.be/IRZCQO18QAI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c264cc",
   "metadata": {},
   "source": [
    "## Overview\n",
    "- Translate network diagrams into code by defining layers, activations, and loss functions.\n",
    "- Reinforce how modular implementations ease experimentation with architecture changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e093e54a",
   "metadata": {},
   "source": [
    "## Key ideas\n",
    "- **Layer abstraction:** encapsulate weight initialization, forward, and backward operations.\n",
    "- **Activation functions:** ReLU, tanh, and sigmoid shape gradient flow.\n",
    "- **Loss coupling:** classification uses cross-entropy, regression uses mean squared error.\n",
    "- **Testing:** unit tests on tiny inputs confirm gradients and shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3b3b5b",
   "metadata": {},
   "source": [
    "## Demo\n",
    "Build a minimal neural network class in NumPy with forward/backward methods and train it on XOR, mirroring the coding details in the lecture (https://youtu.be/wYD0WVrNa1I)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "313fdccf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T01:26:14.959286Z",
     "iopub.status.busy": "2025-10-29T01:26:14.959017Z",
     "iopub.status.idle": "2025-10-29T01:26:15.763736Z",
     "shell.execute_reply": "2025-10-29T01:26:15.763446Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 200 | loss 0.6914 | acc 0.500\n",
      "epoch 400 | loss 0.6659 | acc 0.500\n",
      "epoch 600 | loss 0.2218 | acc 1.000\n",
      "epoch 800 | loss 0.0530 | acc 1.000\n",
      "\n",
      "Final predictions: [0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=float)\n",
    "y = np.array([0, 1, 1, 0])\n",
    "y_onehot = np.eye(2)[y]\n",
    "\n",
    "rng = np.random.default_rng(2)\n",
    "W1 = rng.normal(scale=0.5, size=(2, 4))\n",
    "B1 = np.zeros(4)\n",
    "W2 = rng.normal(scale=0.5, size=(4, 2))\n",
    "B2 = np.zeros(2)\n",
    "\n",
    "lr = 0.5\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_grad(x):\n",
    "    sx = sigmoid(x)\n",
    "    return sx * (1 - sx)\n",
    "\n",
    "for epoch in range(1, 801):\n",
    "    z1 = X @ W1 + B1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = a1 @ W2 + B2\n",
    "    logits = z2\n",
    "    exp = np.exp(logits - logits.max(axis=1, keepdims=True))\n",
    "    probs = exp / exp.sum(axis=1, keepdims=True)\n",
    "    loss = -np.mean(np.sum(y_onehot * np.log(probs + 1e-8), axis=1))\n",
    "\n",
    "    grad_logits = (probs - y_onehot) / len(X)\n",
    "    grad_W2 = a1.T @ grad_logits\n",
    "    grad_B2 = grad_logits.sum(axis=0)\n",
    "\n",
    "    grad_a1 = grad_logits @ W2.T\n",
    "    grad_z1 = grad_a1 * sigmoid_grad(z1)\n",
    "    grad_W1 = X.T @ grad_z1\n",
    "    grad_B1 = grad_z1.sum(axis=0)\n",
    "\n",
    "    W2 -= lr * grad_W2\n",
    "    B2 -= lr * grad_B2\n",
    "    W1 -= lr * grad_W1\n",
    "    B1 -= lr * grad_B1\n",
    "\n",
    "    if epoch % 200 == 0:\n",
    "        preds = np.argmax(probs, axis=1)\n",
    "        acc = accuracy_score(y, preds)\n",
    "        print(f\"epoch {epoch:3d} | loss {loss:.4f} | acc {acc:.3f}\")\n",
    "\n",
    "final_logits = sigmoid(X @ W1 + B1) @ W2 + B2\n",
    "preds = np.argmax(np.exp(final_logits), axis=1)\n",
    "print()\n",
    "print('Final predictions:', preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f26e297",
   "metadata": {},
   "source": [
    "## Try it\n",
    "- Modify the demo\n",
    "- Add a tiny dataset or counter-example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16acdb91",
   "metadata": {},
   "source": [
    "## References\n",
    "- [Eisenstein 4.2](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Multiclass lecture note](https://www.cs.utexas.edu/~gdurrett/courses/online-course/multiclass.pdf)\n",
    "- [A large annotated corpus for learning natural language inference](https://www.aclweb.org/anthology/D15-1075/)\n",
    "- [Authorship Attribution of Micro-Messages](https://www.aclweb.org/anthology/D13-1193/)\n",
    "- [50 Years of Test (Un)fairness: Lessons for Machine Learning](https://arxiv.org/pdf/1811.10104.pdf)\n",
    "- [[Article] Amazon scraps secret AI recruiting tool that showed bias against women](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G)\n",
    "- [[Blog] Neural Networks, Manifolds, and Topology](http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/)\n",
    "- [Eisenstein Chapter 3.1-3.3](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Dropout: a simple way to prevent neural networks from overfitting](https://dl.acm.org/doi/10.5555/2627435.2670313)\n",
    "- [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)\n",
    "- [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980)\n",
    "- [The Marginal Value of Adaptive Gradient Methods in Machine Learning](https://papers.nips.cc/paper/2017/hash/81b3833e2504647f9d794f7d7b9bf341-Abstract.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9f82c1",
   "metadata": {},
   "source": [
    "*Links only; we do not redistribute slides or papers.*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
