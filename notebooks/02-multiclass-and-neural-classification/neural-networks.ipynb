{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e79a010",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "- ðŸ“º **Video:** [https://youtu.be/DU_p-RBy5gM](https://youtu.be/DU_p-RBy5gM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1bc906",
   "metadata": {},
   "source": [
    "## Overview\n",
    "- Motivate neural networks as flexible function approximators that learn hierarchical features.\n",
    "- See how depth, activation functions, and capacity interact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0193a0f5",
   "metadata": {},
   "source": [
    "## Key ideas\n",
    "- **Nonlinearity:** stacking linear layers without nonlinearities collapses to a single linear map.\n",
    "- **Capacity:** width and depth increase expressiveness but risk overfitting.\n",
    "- **Optimization:** gradient-based methods enable large-scale training.\n",
    "- **Generalization:** regularization and data augmentation tame over-parameterized models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51105f67",
   "metadata": {},
   "source": [
    "## Demo\n",
    "Fit a shallow and a deeper network on the same data to compare their capacity, echoing the lecture (https://youtu.be/bHnf4UxwZls) on why depth matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "253bc8b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T01:26:19.946853Z",
     "iopub.status.busy": "2025-10-29T01:26:19.946632Z",
     "iopub.status.idle": "2025-10-29T01:26:21.010707Z",
     "shell.execute_reply": "2025-10-29T01:26:21.010428Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shallow epoch 200 | loss 0.2841 | acc 0.878\n",
      "shallow epoch 400 | loss 0.2372 | acc 0.896\n",
      "deep epoch 200 | loss 0.1544 | acc 0.940\n",
      "deep epoch 400 | loss 0.1331 | acc 0.938\n",
      "Final shallow accuracy: 0.896\n",
      "Final deep accuracy: 0.94\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "rng = np.random.default_rng(8)\n",
    "X, y = make_moons(n_samples=500, noise=0.25, random_state=8)\n",
    "y_onehot = np.eye(2)[y]\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def forward(X, layers):\n",
    "    activations = X\n",
    "    caches = []\n",
    "    for W, B in layers[:-1]:\n",
    "        z = activations @ W + B\n",
    "        activations = relu(z)\n",
    "        caches.append((z, activations))\n",
    "    W_last, B_last = layers[-1]\n",
    "    logits = activations @ W_last + B_last\n",
    "    exp = np.exp(logits - logits.max(axis=1, keepdims=True))\n",
    "    probs = exp / exp.sum(axis=1, keepdims=True)\n",
    "    return probs\n",
    "\n",
    "layers_shallow = [\n",
    "    (rng.normal(scale=0.3, size=(2, 16)), np.zeros(16)),\n",
    "    (rng.normal(scale=0.3, size=(16, 2)), np.zeros(2))\n",
    "]\n",
    "\n",
    "layers_deep = [\n",
    "    (rng.normal(scale=0.3, size=(2, 16)), np.zeros(16)),\n",
    "    (rng.normal(scale=0.3, size=(16, 16)), np.zeros(16)),\n",
    "    (rng.normal(scale=0.3, size=(16, 2)), np.zeros(2))\n",
    "]\n",
    "\n",
    "def train(layers, lr=0.3, epochs=400):\n",
    "    W1, B1 = layers[0]\n",
    "    W_out, B_out = layers[-1]\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        if len(layers) == 2:\n",
    "            z1 = X @ layers[0][0] + layers[0][1]\n",
    "            a1 = relu(z1)\n",
    "            logits = a1 @ layers[1][0] + layers[1][1]\n",
    "            hidden_stack = [(z1, a1)]\n",
    "        else:\n",
    "            z1 = X @ layers[0][0] + layers[0][1]\n",
    "            a1 = relu(z1)\n",
    "            z2 = a1 @ layers[1][0] + layers[1][1]\n",
    "            a2 = relu(z2)\n",
    "            logits = a2 @ layers[2][0] + layers[2][1]\n",
    "            hidden_stack = [(z1, a1), (z2, a2)]\n",
    "\n",
    "        exp = np.exp(logits - logits.max(axis=1, keepdims=True))\n",
    "        probs = exp / exp.sum(axis=1, keepdims=True)\n",
    "        loss = -np.mean(np.sum(y_onehot * np.log(probs + 1e-8), axis=1))\n",
    "        grad_logits = (probs - y_onehot) / len(X)\n",
    "\n",
    "        if len(layers) == 2:\n",
    "            grad_W2 = hidden_stack[-1][1].T @ grad_logits\n",
    "            grad_B2 = grad_logits.sum(axis=0)\n",
    "            grad_hidden = grad_logits @ layers[1][0].T\n",
    "            grad_z1 = grad_hidden * (hidden_stack[-1][0] > 0)\n",
    "            grad_W1 = X.T @ grad_z1\n",
    "            grad_B1 = grad_z1.sum(axis=0)\n",
    "\n",
    "            layers[1] = (layers[1][0] - lr * grad_W2, layers[1][1] - lr * grad_B2)\n",
    "            layers[0] = (layers[0][0] - lr * grad_W1, layers[0][1] - lr * grad_B1)\n",
    "        else:\n",
    "            grad_W3 = hidden_stack[-1][1].T @ grad_logits\n",
    "            grad_B3 = grad_logits.sum(axis=0)\n",
    "            grad_a2 = grad_logits @ layers[2][0].T\n",
    "            grad_z2 = grad_a2 * (hidden_stack[-1][0] > 0)\n",
    "            grad_W2 = hidden_stack[0][1].T @ grad_z2\n",
    "            grad_B2 = grad_z2.sum(axis=0)\n",
    "            grad_a1 = grad_z2 @ layers[1][0].T\n",
    "            grad_z1 = grad_a1 * (hidden_stack[0][0] > 0)\n",
    "            grad_W1 = X.T @ grad_z1\n",
    "            grad_B1 = grad_z1.sum(axis=0)\n",
    "\n",
    "            layers[2] = (layers[2][0] - lr * grad_W3, layers[2][1] - lr * grad_B3)\n",
    "            layers[1] = (layers[1][0] - lr * grad_W2, layers[1][1] - lr * grad_B2)\n",
    "            layers[0] = (layers[0][0] - lr * grad_W1, layers[0][1] - lr * grad_B1)\n",
    "\n",
    "        if epoch % 200 == 0:\n",
    "            preds = np.argmax(probs, axis=1)\n",
    "            acc = accuracy_score(y, preds)\n",
    "            print(f\"{'deep' if len(layers)==3 else 'shallow'} epoch {epoch:3d} | loss {loss:.4f} | acc {acc:.3f}\")\n",
    "\n",
    "train(layers_shallow)\n",
    "train(layers_deep)\n",
    "\n",
    "for label, layers in [('shallow', layers_shallow), ('deep', layers_deep)]:\n",
    "    if len(layers) == 2:\n",
    "        probs = forward(X, layers)\n",
    "    else:\n",
    "        probs = forward(X, layers)\n",
    "    preds = np.argmax(probs, axis=1)\n",
    "    print(f\"Final {label} accuracy:\", accuracy_score(y, preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca224792",
   "metadata": {},
   "source": [
    "## Try it\n",
    "- Modify the demo\n",
    "- Add a tiny dataset or counter-example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7833c20b",
   "metadata": {},
   "source": [
    "## References\n",
    "- [Eisenstein 4.2](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Multiclass lecture note](https://www.cs.utexas.edu/~gdurrett/courses/online-course/multiclass.pdf)\n",
    "- [A large annotated corpus for learning natural language inference](https://www.aclweb.org/anthology/D15-1075/)\n",
    "- [Authorship Attribution of Micro-Messages](https://www.aclweb.org/anthology/D13-1193/)\n",
    "- [50 Years of Test (Un)fairness: Lessons for Machine Learning](https://arxiv.org/pdf/1811.10104.pdf)\n",
    "- [[Article] Amazon scraps secret AI recruiting tool that showed bias against women](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G)\n",
    "- [[Blog] Neural Networks, Manifolds, and Topology](http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/)\n",
    "- [Eisenstein Chapter 3.1-3.3](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Dropout: a simple way to prevent neural networks from overfitting](https://dl.acm.org/doi/10.5555/2627435.2670313)\n",
    "- [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)\n",
    "- [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980)\n",
    "- [The Marginal Value of Adaptive Gradient Methods in Machine Learning](https://papers.nips.cc/paper/2017/hash/81b3833e2504647f9d794f7d7b9bf341-Abstract.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb399ca",
   "metadata": {},
   "source": [
    "*Links only; we do not redistribute slides or papers.*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
