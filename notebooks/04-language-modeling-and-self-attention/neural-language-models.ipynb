{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8396be8d",
   "metadata": {},
   "source": [
    "# Neural Language Models\n",
    "\n",
    "- ðŸ“º **Video:** [https://youtu.be/59NrmwAdOWA](https://youtu.be/59NrmwAdOWA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea47e24",
   "metadata": {},
   "source": [
    "## Overview\n",
    "- Show how neural language models learn embeddings and nonlinear predictors for next-token probabilities.\n",
    "- Highlight advantages over count-based models in capturing long contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a910ff",
   "metadata": {},
   "source": [
    "## Key ideas\n",
    "- **Distributed representations:** neural LMs jointly learn embeddings and predictions.\n",
    "- **Context windows:** feed-forward neural LMs use concatenated embeddings as input.\n",
    "- **Optimization:** train with stochastic gradient descent using cross-entropy loss.\n",
    "- **Generalization:** neural LMs share parameters across contexts, reducing sparsity issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00cf061",
   "metadata": {},
   "source": [
    "## Demo\n",
    "Implement a simple neural language model with embeddings and a hidden layer on a toy corpus, mirroring the lecture (https://youtu.be/pa4eqR8eYp0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "717e882d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T01:28:43.954510Z",
     "iopub.status.busy": "2025-10-29T01:28:43.954055Z",
     "iopub.status.idle": "2025-10-29T01:28:44.208916Z",
     "shell.execute_reply": "2025-10-29T01:28:44.208324Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100 | loss 0.3768\n",
      "epoch 200 | loss 0.3337\n",
      "epoch 300 | loss 0.3231\n",
      "\n",
      "Next-word distribution for context \"the cat\":\n",
      "P(</s>|the cat) = 0.633\n",
      "P(<s>|the cat) = 0.000\n",
      "P(ate|the cat) = 0.000\n",
      "P(cat|the cat) = 0.000\n",
      "P(chased|the cat) = 0.005\n",
      "P(cheese|the cat) = 0.003\n",
      "P(dog|the cat) = 0.000\n",
      "P(mat|the cat) = 0.000\n",
      "P(mouse|the cat) = 0.000\n",
      "P(on|the cat) = 0.000\n",
      "P(sat|the cat) = 0.359\n",
      "P(the|the cat) = 0.000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "corpus = [\n",
    "    'the cat sat on the mat',\n",
    "    'the dog chased the cat',\n",
    "    'the mouse ate the cheese'\n",
    "]\n",
    "\n",
    "vocab = sorted(set(['<s>'] + ' '.join(corpus).split() + ['</s>']))\n",
    "word_to_id = {word: idx for idx, word in enumerate(vocab)}\n",
    "context_size = 2\n",
    "embed_dim = 8\n",
    "hidden_dim = 16\n",
    "\n",
    "sequences = []\n",
    "for sentence in corpus:\n",
    "    tokens = sentence.split() + ['</s>']\n",
    "    padded = ['<s>'] * context_size + tokens\n",
    "    for i in range(context_size, len(padded)):\n",
    "        context = padded[i-context_size:i]\n",
    "        target = padded[i]\n",
    "        sequences.append(([word_to_id[c] for c in context], word_to_id[target]))\n",
    "\n",
    "rng = np.random.default_rng(4)\n",
    "embeddings = rng.normal(scale=0.1, size=(len(vocab), embed_dim))\n",
    "W1 = rng.normal(scale=0.1, size=(context_size * embed_dim, hidden_dim))\n",
    "B1 = np.zeros(hidden_dim)\n",
    "W2 = rng.normal(scale=0.1, size=(hidden_dim, len(vocab)))\n",
    "B2 = np.zeros(len(vocab))\n",
    "\n",
    "lr = 0.1\n",
    "for epoch in range(1, 301):\n",
    "    total_loss = 0.0\n",
    "    rng.shuffle(sequences)\n",
    "    for context_ids, target_id in sequences:\n",
    "        context_vec = embeddings[context_ids].reshape(-1)\n",
    "        h = np.tanh(context_vec @ W1 + B1)\n",
    "        logits = h @ W2 + B2\n",
    "        exp = np.exp(logits - logits.max())\n",
    "        probs = exp / exp.sum()\n",
    "        loss = -np.log(probs[target_id] + 1e-8)\n",
    "        total_loss += loss\n",
    "\n",
    "        grad_logits = probs\n",
    "        grad_logits[target_id] -= 1\n",
    "        grad_W2 = np.outer(h, grad_logits)\n",
    "        grad_B2 = grad_logits\n",
    "        grad_h = grad_logits @ W2.T\n",
    "        grad_pre = grad_h * (1 - h ** 2)\n",
    "        grad_W1 = np.outer(context_vec, grad_pre)\n",
    "        grad_B1 = grad_pre\n",
    "        grad_context = grad_pre @ W1.T\n",
    "        grad_context = grad_context.reshape(context_size, embed_dim)\n",
    "\n",
    "        W2 -= lr * grad_W2\n",
    "        B2 -= lr * grad_B2\n",
    "        W1 -= lr * grad_W1\n",
    "        B1 -= lr * grad_B1\n",
    "        for idx, emb_grad in zip(context_ids, grad_context):\n",
    "            embeddings[idx] -= lr * emb_grad\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"epoch {epoch:3d} | loss {total_loss/len(sequences):.4f}\")\n",
    "\n",
    "# Evaluate next-word probabilities\n",
    "context = ['the', 'cat']\n",
    "context_ids = [word_to_id[c] for c in context]\n",
    "context_vec = embeddings[context_ids].reshape(-1)\n",
    "h = np.tanh(context_vec @ W1 + B1)\n",
    "logits = h @ W2 + B2\n",
    "probs = np.exp(logits - logits.max())\n",
    "probs /= probs.sum()\n",
    "\n",
    "print()\n",
    "print('Next-word distribution for context \"the cat\":')\n",
    "for word, idx in word_to_id.items():\n",
    "    print(f\"P({word}|{' '.join(context)}) = {probs[idx]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a457225e",
   "metadata": {},
   "source": [
    "## Try it\n",
    "- Modify the demo\n",
    "- Add a tiny dataset or counter-example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e351d51",
   "metadata": {},
   "source": [
    "## References\n",
    "- [Eisenstein 6.1](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Eisenstein 6.2](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Eisenstein 6.4](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Eisenstein 6.3](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [[Blog] Understanding LSTMs](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "- [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)\n",
    "- [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "- [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "- [[Blog] The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
    "- [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "- [Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation](https://arxiv.org/abs/2108.12409)\n",
    "- [The Impact of Positional Encoding on Length Generalization in Transformers](https://arxiv.org/abs/2305.19466)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a383c471",
   "metadata": {},
   "source": [
    "*Links only; we do not redistribute slides or papers.*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
