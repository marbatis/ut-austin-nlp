{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bd3d4bf",
   "metadata": {},
   "source": [
    "# n-gram LMs\n",
    "\n",
    "- ðŸ“º **Video:** [https://youtu.be/J-yHbD8LYCM](https://youtu.be/J-yHbD8LYCM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebf7e9b",
   "metadata": {},
   "source": [
    "## Overview\n",
    "- Introduce n-gram language models that predict the next token from fixed-length histories.\n",
    "- Understand data sparsity challenges and motivations for smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392f57ff",
   "metadata": {},
   "source": [
    "## Key ideas\n",
    "- **Markov assumption:** condition next word on the previous n-1 words.\n",
    "- **Count-based models:** maximum likelihood counts work well with abundant data.\n",
    "- **Sparsity:** unseen n-grams receive zero probability without smoothing.\n",
    "- **Applications:** n-gram LMs score sentences and provide baselines for speech and MT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4602fc1d",
   "metadata": {},
   "source": [
    "## Demo\n",
    "Estimate a trigram language model on a mini corpus and sample sentences, connecting to the lecture (https://youtu.be/-9urhJp9G68)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0085b97b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T01:28:43.128088Z",
     "iopub.status.busy": "2025-10-29T01:28:43.127853Z",
     "iopub.status.idle": "2025-10-29T01:28:43.197698Z",
     "shell.execute_reply": "2025-10-29T01:28:43.196883Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of \"the cat chased\": 0.3333333333333333\n",
      "Sampled: \n",
      "Sampled: \n",
      "Sampled: \n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "corpus = [\n",
    "    'the cat sat on the mat',\n",
    "    'the dog chased the cat',\n",
    "    'the cat chased the mouse',\n",
    "    'the mouse ate the cheese',\n",
    "    'the dog sat on the rug'\n",
    "]\n",
    "\n",
    "trigram_counts = defaultdict(int)\n",
    "bigram_counts = defaultdict(int)\n",
    "for sentence in corpus:\n",
    "    tokens = ['<s>', '<s>'] + sentence.split() + ['</s>']\n",
    "    for i in range(2, len(tokens)):\n",
    "        trigram = (tokens[i-2], tokens[i-1], tokens[i])\n",
    "        bigram = (tokens[i-2], tokens[i-1])\n",
    "        trigram_counts[trigram] += 1\n",
    "        bigram_counts[bigram] += 1\n",
    "\n",
    "def trigram_prob(w1, w2, w3):\n",
    "    count = trigram_counts[(w1, w2, w3)]\n",
    "    base = bigram_counts[(w1, w2)]\n",
    "    if base == 0:\n",
    "        return 0.0\n",
    "    return count / base\n",
    "\n",
    "def sample_sentence(max_len=15):\n",
    "    w1, w2 = '<s>', '<s>'\n",
    "    sentence = []\n",
    "    for _ in range(max_len):\n",
    "        vocab = sorted({token for (_, _, token) in trigram_counts.keys() if _[0] == w1 and _[1] == w2})\n",
    "        if not vocab:\n",
    "            break\n",
    "        probs = np.array([trigram_prob(w1, w2, w3) for w3 in vocab])\n",
    "        probs /= probs.sum()\n",
    "        w3 = np.random.choice(vocab, p=probs)\n",
    "        if w3 == '</s>':\n",
    "            break\n",
    "        sentence.append(w3)\n",
    "        w1, w2 = w2, w3\n",
    "    return ' '.join(sentence)\n",
    "\n",
    "print('Probability of \"the cat chased\":', trigram_prob('the', 'cat', 'chased'))\n",
    "for _ in range(3):\n",
    "    print('Sampled:', sample_sentence())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f792ff66",
   "metadata": {},
   "source": [
    "## Try it\n",
    "- Modify the demo\n",
    "- Add a tiny dataset or counter-example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ccf079",
   "metadata": {},
   "source": [
    "## References\n",
    "- [Eisenstein 6.1](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Eisenstein 6.2](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Eisenstein 6.4](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Eisenstein 6.3](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [[Blog] Understanding LSTMs](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "- [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)\n",
    "- [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "- [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "- [[Blog] The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
    "- [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "- [Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation](https://arxiv.org/abs/2108.12409)\n",
    "- [The Impact of Positional Encoding on Length Generalization in Transformers](https://arxiv.org/abs/2305.19466)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860d7062",
   "metadata": {},
   "source": [
    "*Links only; we do not redistribute slides or papers.*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
