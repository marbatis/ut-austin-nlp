{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac5e9193",
   "metadata": {},
   "source": [
    "# Multi-Head Self-Attention\n",
    "\n",
    "- üì∫ **Video:** [https://youtu.be/nHXrdLMo8Uk](https://youtu.be/nHXrdLMo8Uk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8e7624",
   "metadata": {},
   "source": [
    "## Overview\n",
    "Expands on self-attention by introducing the concept of multiple attention heads, as implemented in the Transformer model The video explains that instead of computing a single attention for each word, the Transformer uses h parallel attention heads (commonly 8 or so), each with its own projection matrices for queries, keys, and values. This means each head can focus on different aspects of the word relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ee6e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "random.seed(0)\n",
    "CI = os.environ.get('CI') == 'true'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e0e183",
   "metadata": {},
   "source": [
    "## Key ideas\n",
    "- For example, in a translation context, one head might align pronouns, another might han verb tense, another might connect noun-adjective pairs.\n",
    "- In an English self-attention context, one head for a word might attend strongly to the subject for agreement, another head might attend to an object for semantic role, etc.\n",
    "- The lecturer likely describes how multi-head attention is computed: each head yields its own context vector (output) for each word, and then these outputs are concatenated and linearly transformed to produce the final representation for that word in that layer This allows the model to capture different types of relationships simultaneously.\n",
    "- The reading ‚ÄúAttention Is All You Need‚Äù is referenced heavily here; perhaps the video walks through one of the figures from that paper, showing multi-head attention schematically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9b4245",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef00c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaled dot-product attention (toy)\n",
    "import numpy as np\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "    x = x - x.max(axis=axis, keepdims=True)\n",
    "    e = np.exp(x)\n",
    "    return e / e.sum(axis=axis, keepdims=True)\n",
    "\n",
    "np.random.seed(0)\n",
    "Q = np.random.randn(3, 4)  # 3 queries, dim 4\n",
    "K = np.random.randn(5, 4)  # 5 keys, dim 4\n",
    "V = np.random.randn(5, 6)  # 5 values, dim 6\n",
    "\n",
    "scores = Q @ K.T / np.sqrt(Q.shape[-1])  # (3,5)\n",
    "weights = softmax(scores, axis=-1)       # (3,5)\n",
    "out = weights @ V                        # (3,6)\n",
    "print(\"weights.shape:\", weights.shape, \"out.shape:\", out.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e0f6d2",
   "metadata": {},
   "source": [
    "## Try it\n",
    "- Modify the demo\n",
    "- Add a tiny dataset or counter-example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c14bdb3",
   "metadata": {},
   "source": [
    "## References\n",
    "- [Eisenstein 6.1](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Eisenstein 6.2](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Eisenstein 6.4](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Eisenstein 6.3](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [[Blog] Understanding LSTMs](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "- [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)\n",
    "- [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "- [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "- [[Blog] The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
    "- [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "- [Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation](https://arxiv.org/abs/2108.12409)\n",
    "- [The Impact of Positional Encoding on Length Generalization in Transformers](https://arxiv.org/abs/2305.19466)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cb57ef",
   "metadata": {},
   "source": [
    "*Links only; we do not redistribute slides or papers.*"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
