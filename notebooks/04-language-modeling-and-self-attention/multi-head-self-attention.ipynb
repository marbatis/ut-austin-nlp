{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac5e9193",
   "metadata": {},
   "source": [
    "# Multi-Head Self-Attention\n",
    "\n",
    "- ðŸ“º **Video:** [https://youtu.be/nHXrdLMo8Uk](https://youtu.be/nHXrdLMo8Uk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8e7624",
   "metadata": {},
   "source": [
    "## Overview\n",
    "- Show how multi-head self-attention allows the model to focus on different relational patterns simultaneously.\n",
    "- Explain head concatenation and projection back to the model dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e0e183",
   "metadata": {},
   "source": [
    "## Key ideas\n",
    "- **Multiple heads:** each head uses different learned projections to capture varied dependencies.\n",
    "- **Concatenation:** head outputs are concatenated then linearly mixed.\n",
    "- **Dimensionality:** head dimension times number of heads equals model dimension.\n",
    "- **Interpretability:** different heads often specialize (syntax, coreference, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9b4245",
   "metadata": {},
   "source": [
    "## Demo\n",
    "Implement a two-head self-attention block on toy embeddings to illustrate the computation pipeline described in the lecture (https://youtu.be/6FM2ctkEoMc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ef00c07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T01:28:42.398331Z",
     "iopub.status.busy": "2025-10-29T01:28:42.398114Z",
     "iopub.status.idle": "2025-10-29T01:28:42.471602Z",
     "shell.execute_reply": "2025-10-29T01:28:42.470579Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head 0 output:\n",
      "[[-0.13502184  0.16859793]\n",
      " [-0.13458499  0.16709715]\n",
      " [-0.13486361  0.16837749]]\n",
      "\n",
      "Head 1 output:\n",
      "[[0.41382381 0.51585003]\n",
      " [0.41388256 0.51598671]\n",
      " [0.41415734 0.51665625]]\n",
      "\n",
      "Multi-head combination:\n",
      "[[-0.11650333 -0.15291629  0.05670701 -0.11995107]\n",
      " [-0.11619959 -0.15312817  0.05761014 -0.12044207]\n",
      " [-0.1164723  -0.15317532  0.05724618 -0.12031059]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([\n",
    "    [0.5, 0.1, 0.3, 0.2],\n",
    "    [0.2, 0.4, 0.1, 0.5],\n",
    "    [0.7, 0.0, 0.2, 0.4]\n",
    "])\n",
    "model_dim = X.shape[1]\n",
    "num_heads = 2\n",
    "head_dim = model_dim // num_heads\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "W_q = rng.normal(scale=0.4, size=(model_dim, model_dim))\n",
    "W_k = rng.normal(scale=0.4, size=(model_dim, model_dim))\n",
    "W_v = rng.normal(scale=0.4, size=(model_dim, model_dim))\n",
    "W_o = rng.normal(scale=0.4, size=(model_dim, model_dim))\n",
    "\n",
    "Q = X @ W_q\n",
    "K = X @ W_k\n",
    "V = X @ W_v\n",
    "\n",
    "heads = []\n",
    "for h in range(num_heads):\n",
    "    q_h = Q[:, h*head_dim:(h+1)*head_dim]\n",
    "    k_h = K[:, h*head_dim:(h+1)*head_dim]\n",
    "    v_h = V[:, h*head_dim:(h+1)*head_dim]\n",
    "    scale = np.sqrt(head_dim)\n",
    "    logits = q_h @ k_h.T / scale\n",
    "    weights = np.exp(logits - logits.max(axis=-1, keepdims=True))\n",
    "    weights /= weights.sum(axis=-1, keepdims=True)\n",
    "    heads.append(weights @ v_h)\n",
    "\n",
    "concat = np.concatenate(heads, axis=-1)\n",
    "output = concat @ W_o\n",
    "\n",
    "print('Head 0 output:')\n",
    "print(heads[0])\n",
    "print()\n",
    "print('Head 1 output:')\n",
    "print(heads[1])\n",
    "print()\n",
    "print('Multi-head combination:')\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e0f6d2",
   "metadata": {},
   "source": [
    "## Try it\n",
    "- Modify the demo\n",
    "- Add a tiny dataset or counter-example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c14bdb3",
   "metadata": {},
   "source": [
    "## References\n",
    "- [Eisenstein 6.1](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Eisenstein 6.2](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Eisenstein 6.4](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Eisenstein 6.3](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [[Blog] Understanding LSTMs](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "- [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)\n",
    "- [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "- [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "- [[Blog] The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
    "- [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "- [Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation](https://arxiv.org/abs/2108.12409)\n",
    "- [The Impact of Positional Encoding on Length Generalization in Transformers](https://arxiv.org/abs/2305.19466)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cb57ef",
   "metadata": {},
   "source": [
    "*Links only; we do not redistribute slides or papers.*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
