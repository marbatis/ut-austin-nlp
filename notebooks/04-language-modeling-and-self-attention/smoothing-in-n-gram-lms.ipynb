{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddbcee90",
   "metadata": {},
   "source": [
    "# Smoothing in n-gram LMs\n",
    "\n",
    "- ðŸ“º **Video:** [https://youtu.be/Yfug5eIQh5w](https://youtu.be/Yfug5eIQh5w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ebe5a8",
   "metadata": {},
   "source": [
    "## Overview\n",
    "- Tackle zero probabilities in n-gram models using add-k and interpolated smoothing.\n",
    "- Interpret how smoothing redistributes mass to unseen events."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34837e6",
   "metadata": {},
   "source": [
    "## Key ideas\n",
    "- **Additive smoothing:** add pseudo-counts to avoid zeros.\n",
    "- **Interpolation:** blend higher- and lower-order models for robustness.\n",
    "- **Discounting:** subtract mass from seen events to allocate to unseen ones.\n",
    "- **Perplexity impact:** smoothing often improves evaluation on held-out data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894e5ce5",
   "metadata": {},
   "source": [
    "## Demo\n",
    "Compare maximum-likelihood and add-one smoothed probabilities on sparse trigrams to mirror the lecture (https://youtu.be/rXySwxZMaf0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f927ae6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T01:28:47.259619Z",
     "iopub.status.busy": "2025-10-29T01:28:47.259362Z",
     "iopub.status.idle": "2025-10-29T01:28:47.271875Z",
     "shell.execute_reply": "2025-10-29T01:28:47.271470Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(chased|the cat) MLE=0.3333 | add-one=0.1333\n",
      "P(ate|the cat) MLE=0.0000 | add-one=0.0667\n",
      "P(on|dog sat) MLE=1.0000 | add-one=0.1538\n",
      "P(the|sat on) MLE=1.0000 | add-one=0.2143\n",
      "P(the|mouse ate) MLE=1.0000 | add-one=0.1538\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "corpus = [\n",
    "    'the cat sat on the mat',\n",
    "    'the dog chased the cat',\n",
    "    'the cat chased the mouse',\n",
    "    'the mouse ate the cheese',\n",
    "    'the dog sat on the rug'\n",
    "]\n",
    "\n",
    "vocab = sorted(set(' '.join(corpus).split())) + ['</s>']\n",
    "trigram_counts = defaultdict(int)\n",
    "bigram_counts = defaultdict(int)\n",
    "for sentence in corpus:\n",
    "    tokens = ['<s>', '<s>'] + sentence.split() + ['</s>']\n",
    "    for i in range(2, len(tokens)):\n",
    "        trigram = (tokens[i-2], tokens[i-1], tokens[i])\n",
    "        bigram = (tokens[i-2], tokens[i-1])\n",
    "        trigram_counts[trigram] += 1\n",
    "        bigram_counts[bigram] += 1\n",
    "\n",
    "def mle_prob(w1, w2, w3):\n",
    "    count = trigram_counts[(w1, w2, w3)]\n",
    "    base = bigram_counts[(w1, w2)]\n",
    "    return count / base if base else 0.0\n",
    "\n",
    "def add_one_prob(w1, w2, w3):\n",
    "    count = trigram_counts[(w1, w2, w3)] + 1\n",
    "    base = bigram_counts[(w1, w2)] + len(vocab)\n",
    "    return count / base\n",
    "\n",
    "examples = [\n",
    "    ('the', 'cat', 'chased'),\n",
    "    ('the', 'cat', 'ate'),\n",
    "    ('dog', 'sat', 'on'),\n",
    "    ('sat', 'on', 'the'),\n",
    "    ('mouse', 'ate', 'the')\n",
    "]\n",
    "\n",
    "for w1, w2, w3 in examples:\n",
    "    print(f\"P({w3}|{w1} {w2}) MLE={mle_prob(w1, w2, w3):.4f} | add-one={add_one_prob(w1, w2, w3):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bbf3ef",
   "metadata": {},
   "source": [
    "## Try it\n",
    "- Modify the demo\n",
    "- Add a tiny dataset or counter-example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c17692b",
   "metadata": {},
   "source": [
    "## References\n",
    "- [Eisenstein 6.1](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Eisenstein 6.2](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Eisenstein 6.4](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Eisenstein 6.3](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [[Blog] Understanding LSTMs](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "- [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)\n",
    "- [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "- [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "- [[Blog] The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
    "- [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "- [Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation](https://arxiv.org/abs/2108.12409)\n",
    "- [The Impact of Positional Encoding on Length Generalization in Transformers](https://arxiv.org/abs/2305.19466)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b294927",
   "metadata": {},
   "source": [
    "*Links only; we do not redistribute slides or papers.*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
