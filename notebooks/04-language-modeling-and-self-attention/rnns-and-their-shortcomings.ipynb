{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de4888ab",
   "metadata": {},
   "source": [
    "# RNNs and their Shortcomings\n",
    "\n",
    "- ðŸ“º **Video:** [https://youtu.be/xvnnA04JVQo](https://youtu.be/xvnnA04JVQo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ca5578",
   "metadata": {},
   "source": [
    "## Overview\n",
    "- Review recurrent neural networks for sequential modeling and where they struggle.\n",
    "- Connect vanishing/exploding gradients to difficulties capturing long-range dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3319eef0",
   "metadata": {},
   "source": [
    "## Key ideas\n",
    "- **Recurrence:** hidden state summarizes previous tokens.\n",
    "- **Gradient issues:** repeated multiplication by Jacobians can shrink or explode norms.\n",
    "- **Long-range dependencies:** simple RNNs forget distant context without gating.\n",
    "- **Alternatives:** LSTMs, GRUs, and self-attention mitigate these issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7bbde9",
   "metadata": {},
   "source": [
    "## Demo\n",
    "Simulate gradient propagation through a simple recurrent matrix to show vanishing/exploding effects discussed in the lecture (https://youtu.be/I-8XM1SkJOI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e76f0a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T01:28:45.653640Z",
     "iopub.status.busy": "2025-10-29T01:28:45.653327Z",
     "iopub.status.idle": "2025-10-29T01:28:45.727408Z",
     "shell.execute_reply": "2025-10-29T01:28:45.726160Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden state after 20 steps: [0.00605599 0.00302811]\n",
      "Norm after 5 steps (vanishing case): 0.032885\n",
      "Norm after 10 steps (vanishing case): 0.000982\n",
      "Norm after 15 steps (vanishing case): 0.000031\n",
      "Norm after 20 steps (vanishing case): 0.000001\n",
      "Norm after 25 steps (vanishing case): 0.000000\n",
      "Norm after 30 steps (vanishing case): 0.000000\n",
      "Norm after 2 steps (exploding case): 2.08\n",
      "Norm after 4 steps (exploding case): 3.21\n",
      "Norm after 6 steps (exploding case): 5.14\n",
      "Norm after 8 steps (exploding case): 8.43\n",
      "Norm after 10 steps (exploding case): 14.03\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "W = np.array([[0.7, 0.2], [0.1, 0.6]])\n",
    "T = 20\n",
    "\n",
    "state = np.array([1.0, 0.0])\n",
    "for t in range(T):\n",
    "    state = np.tanh(W @ state)\n",
    "print('Hidden state after 20 steps:', state)\n",
    "\n",
    "# Gradient propagation with eigenvalues < 1\n",
    "W_grad = np.array([[0.5, 0.0], [0.0, 0.4]])\n",
    "vec = np.ones(2)\n",
    "for t in range(1, 31):\n",
    "    vec = W_grad.T @ vec\n",
    "    if t % 5 == 0:\n",
    "        print(f'Norm after {t} steps (vanishing case): {np.linalg.norm(vec):.6f}')\n",
    "\n",
    "# Exploding gradients example\n",
    "W_explode = np.array([[1.3, 0.0], [0.0, 1.1]])\n",
    "vec = np.ones(2)\n",
    "for t in range(1, 11):\n",
    "    vec = W_explode.T @ vec\n",
    "    if t % 2 == 0:\n",
    "        print(f'Norm after {t} steps (exploding case): {np.linalg.norm(vec):.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaea15cf",
   "metadata": {},
   "source": [
    "## Try it\n",
    "- Modify the demo\n",
    "- Add a tiny dataset or counter-example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26440bb",
   "metadata": {},
   "source": [
    "## References\n",
    "- [Eisenstein 6.1](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Eisenstein 6.2](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Eisenstein 6.4](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Eisenstein 6.3](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [[Blog] Understanding LSTMs](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "- [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)\n",
    "- [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "- [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "- [[Blog] The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
    "- [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "- [Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation](https://arxiv.org/abs/2108.12409)\n",
    "- [The Impact of Positional Encoding on Length Generalization in Transformers](https://arxiv.org/abs/2305.19466)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28de75b6",
   "metadata": {},
   "source": [
    "*Links only; we do not redistribute slides or papers.*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
