{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54cbcefb",
   "metadata": {},
   "source": [
    "# Self-Attention\n",
    "\n",
    "- ðŸ“º **Video:** [https://youtu.be/10l2NXStROU](https://youtu.be/10l2NXStROU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df551a7c",
   "metadata": {},
   "source": [
    "## Overview\n",
    "- Explain self-attention as attention applied within a sequence to relate tokens to one another.\n",
    "- Highlight how self-attention replaces recurrence with parallelizable operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f5600e",
   "metadata": {},
   "source": [
    "## Key ideas\n",
    "- **Shared queries/keys/values:** derived from the same input through learned projections.\n",
    "- **Contextualization:** each token attends to others to build contextual representations.\n",
    "- **Masking:** causal masks prevent leaking future tokens during language modeling.\n",
    "- **Computational benefits:** self-attention enables long-range interactions with fewer sequential steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68127aa3",
   "metadata": {},
   "source": [
    "## Demo\n",
    "Apply scaled dot-product self-attention to a sequence of token embeddings and show how masking restricts look-ahead, mirroring the lecture (https://youtu.be/VYBd44C1rBw)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b0f6a41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T01:28:46.510023Z",
     "iopub.status.busy": "2025-10-29T01:28:46.509768Z",
     "iopub.status.idle": "2025-10-29T01:28:46.581376Z",
     "shell.execute_reply": "2025-10-29T01:28:46.580383Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-attention weights (causal):\n",
      "[[1.         0.         0.        ]\n",
      " [0.50565661 0.49434339 0.        ]\n",
      " [0.33667649 0.33371378 0.32960973]]\n",
      "\n",
      "Contextualized representations:\n",
      "[[0.37       0.41      ]\n",
      " [0.33045253 0.31607476]\n",
      " [0.36637558 0.33340999]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([\n",
    "    [0.5, 0.1, 0.3],\n",
    "    [0.2, 0.4, 0.1],\n",
    "    [0.7, 0.0, 0.2]\n",
    "])\n",
    "W_q = np.array([[0.3, 0.6], [0.5, 0.1], [0.2, 0.4]])\n",
    "W_k = np.array([[0.4, 0.2], [0.1, 0.7], [0.3, 0.5]])\n",
    "W_v = np.array([[0.6, 0.3], [0.4, 0.2], [0.1, 0.8]])\n",
    "\n",
    "Q = X @ W_q\n",
    "K = X @ W_k\n",
    "V = X @ W_v\n",
    "\n",
    "scale = np.sqrt(Q.shape[-1])\n",
    "logits = Q @ K.T / scale\n",
    "mask = np.triu(np.ones_like(logits), k=1) * -1e9\n",
    "logits_masked = logits + mask\n",
    "weights = np.exp(logits_masked - logits_masked.max(axis=-1, keepdims=True))\n",
    "weights /= weights.sum(axis=-1, keepdims=True)\n",
    "context = weights @ V\n",
    "\n",
    "print('Self-attention weights (causal):')\n",
    "print(weights)\n",
    "print()\n",
    "print('Contextualized representations:')\n",
    "print(context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf7b554",
   "metadata": {},
   "source": [
    "## Try it\n",
    "- Modify the demo\n",
    "- Add a tiny dataset or counter-example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136dad69",
   "metadata": {},
   "source": [
    "## References\n",
    "- [Eisenstein 6.1](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Eisenstein 6.2](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Eisenstein 6.4](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Eisenstein 6.3](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [[Blog] Understanding LSTMs](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "- [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)\n",
    "- [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "- [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "- [[Blog] The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
    "- [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "- [Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation](https://arxiv.org/abs/2108.12409)\n",
    "- [The Impact of Positional Encoding on Length Generalization in Transformers](https://arxiv.org/abs/2305.19466)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecaf381",
   "metadata": {},
   "source": [
    "*Links only; we do not redistribute slides or papers.*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
