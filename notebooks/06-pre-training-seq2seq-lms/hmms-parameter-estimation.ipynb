{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "055c55ad",
   "metadata": {},
   "source": [
    "# HMMs: Parameter Estimation\n",
    "\n",
    "- ðŸ“º **Video:** [https://youtu.be/dVF7LZkbl9g](https://youtu.be/dVF7LZkbl9g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc0a3bf",
   "metadata": {},
   "source": [
    "## Overview\n",
    "Explains how to learn the probabilities for an HMM, both in supervised and unsupervised contexts For supervised learning (tagged corpus available), it's straightforward: calculate transition probabilities by relative frequency (e.g., P(VB|DT) = count(DT->VB) / count(DT)), and emission probabilities similarly (P(\"dog\"|NN) = count(NN->\"dog\")/count(NN)). The video likely demonstrates that with a small example or formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d72b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "random.seed(0)\n",
    "CI = os.environ.get('CI') == 'true'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1122542b",
   "metadata": {},
   "source": [
    "## Key ideas\n",
    "- It then mentions smoothing for these probabilities because some combinations won't appear in training but could in test (like using add-1 or interpolation on transitions as well).\n",
    "- For unsupervised (no tags), it introduces the Baum-Welch algorithm (an EM algorithm) to iteratively estimate an HMM's parameters by treating the tags as latent variables It probably gives an intuition: start with random or uniform probabilities, then alternate between calculating expected counts of transitions/emissions given the current model (using forward-backward algorithm to soft-count all possible tag paths) and then re-estimating probabilities from those expected counts (E-step and M-step).\n",
    "- Over iterations, it converges to a locally optimal likelihood.\n",
    "- The video might not delve into heavy math, but ensures students know such an algorithm exists to train HMMs without labeled data, albeit the results might not be as good as supervised training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1eb442",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2f0924",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Try the exercises below and follow the linked materials.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2f605a",
   "metadata": {},
   "source": [
    "## Try it\n",
    "- Modify the demo\n",
    "- Add a tiny dataset or counter-example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b77745",
   "metadata": {},
   "source": [
    "## References\n",
    "- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)\n",
    "- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)\n",
    "- [To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks](https://www.aclweb.org/anthology/W19-4302/)\n",
    "- [GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding](https://arxiv.org/pdf/1804.07461.pdf)\n",
    "- [What Does BERT Look At? An Analysis of BERT's Attention](https://arxiv.org/abs/1906.04341)\n",
    "- [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/pdf/1907.11692.pdf)\n",
    "- [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461)\n",
    "- [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683.pdf)\n",
    "- [UnifiedQA: Crossing Format Boundaries With a Single QA System](https://arxiv.org/abs/2005.00700)\n",
    "- [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/pdf/1508.07909.pdf)\n",
    "- [Byte Pair Encoding is Suboptimal for Language Model Pretraining](https://arxiv.org/pdf/2004.03720.pdf)\n",
    "- [Eisenstein 8.1](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Eisenstein 7.1](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Eisenstein 7.4](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Eisenstein 7.4.1](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Eisenstein 7.3](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [TnT - A Statistical Part-of-Speech Tagger](https://arxiv.org/abs/cs/0003055)\n",
    "- [Enriching the Knowledge Sources Used in a Maximum Entropy Part-of-Speech Tagger](https://www.aclweb.org/anthology/W00-1308/)\n",
    "- [Part-of-Speech Tagging from 97% to 100%: Is It Time for Some Linguistics?](https://link.springer.com/chapter/10.1007/978-3-642-19400-9_14)\n",
    "- [Natural Language Processing with Small Feed-Forward Networks](https://www.aclweb.org/anthology/D17-1309.pdf)\n",
    "- [Eisenstein 10.1-10.2](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Eisenstein 10.3-10.4](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Eisenstein 10.3.1](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Accurate Unlexicalized Parsing](https://www.aclweb.org/anthology/P03-1054/)\n",
    "- [Eisenstein 10.5](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Eisenstein 11.1](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Finding Optimal 1-Endpoint-Crossing Trees](https://www.aclweb.org/anthology/Q13-1002/)\n",
    "- [Eisenstein 11.3](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36591575",
   "metadata": {},
   "source": [
    "*Links only; we do not redistribute slides or papers.*"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
