{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98527da8",
   "metadata": {},
   "source": [
    "# Seq2seq Models\n",
    "\n",
    "- ðŸ“º **Video:** [https://youtu.be/TKZkvqb-qpM](https://youtu.be/TKZkvqb-qpM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad894eaa",
   "metadata": {},
   "source": [
    "## Overview\n",
    "Shifts focus from encoder-only models like BERT to sequence-to-sequence models for tasks like translation, summarization, etc. It revisits the concept of an encoder-decoder architecture but now in the context of fully neural models (like we had RNN seq2seq, now Transformers can be seq2seq too)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4c4943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "random.seed(0)\n",
    "CI = os.environ.get('CI') == 'true'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934d25dc",
   "metadata": {},
   "source": [
    "## Key ideas\n",
    "- The video likely outlines the generic structure: an encoder (often a bidirectional model or just BERT-like) processes input text into context representations, and a decoder (usually autoregressive) generates the output text token by token, attending to encoder outputs.\n",
    "- It could give an example: for translation, the encoder reads the French sentence and the decoder produces the English translation, one word at a time, using encoder's info via attention (like in Week 4's attention discussion).\n",
    "- Another example: for question answering generation or dialogue, similar idea.\n",
    "- The video ensures that students recall how seq2seq was historically done with RNNs and attention (Bahdanau 2015), and now basically all those tasks have moved to Transformers (e.g., the Transformer architecture in Vaswani 2017 is inherently seq2seq for translation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397d93f0",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bedf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Try the exercises below and follow the linked materials.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0ee64a",
   "metadata": {},
   "source": [
    "## Try it\n",
    "- Modify the demo\n",
    "- Add a tiny dataset or counter-example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdfb69c",
   "metadata": {},
   "source": [
    "## References\n",
    "- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)\n",
    "- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)\n",
    "- [To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks](https://www.aclweb.org/anthology/W19-4302/)\n",
    "- [GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding](https://arxiv.org/pdf/1804.07461.pdf)\n",
    "- [What Does BERT Look At? An Analysis of BERT's Attention](https://arxiv.org/abs/1906.04341)\n",
    "- [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/pdf/1907.11692.pdf)\n",
    "- [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461)\n",
    "- [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683.pdf)\n",
    "- [UnifiedQA: Crossing Format Boundaries With a Single QA System](https://arxiv.org/abs/2005.00700)\n",
    "- [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/pdf/1508.07909.pdf)\n",
    "- [Byte Pair Encoding is Suboptimal for Language Model Pretraining](https://arxiv.org/pdf/2004.03720.pdf)\n",
    "- [Eisenstein 8.1](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Eisenstein 7.1](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Eisenstein 7.4](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Eisenstein 7.4.1](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Eisenstein 7.3](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [TnT - A Statistical Part-of-Speech Tagger](https://arxiv.org/abs/cs/0003055)\n",
    "- [Enriching the Knowledge Sources Used in a Maximum Entropy Part-of-Speech Tagger](https://www.aclweb.org/anthology/W00-1308/)\n",
    "- [Part-of-Speech Tagging from 97% to 100%: Is It Time for Some Linguistics?](https://link.springer.com/chapter/10.1007/978-3-642-19400-9_14)\n",
    "- [Natural Language Processing with Small Feed-Forward Networks](https://www.aclweb.org/anthology/D17-1309.pdf)\n",
    "- [Eisenstein 10.1-10.2](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Eisenstein 10.3-10.4](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Eisenstein 10.3.1](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Accurate Unlexicalized Parsing](https://www.aclweb.org/anthology/P03-1054/)\n",
    "- [Eisenstein 10.5](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Eisenstein 11.1](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Finding Optimal 1-Endpoint-Crossing Trees](https://www.aclweb.org/anthology/Q13-1002/)\n",
    "- [Eisenstein 11.3](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd359999",
   "metadata": {},
   "source": [
    "*Links only; we do not redistribute slides or papers.*"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
