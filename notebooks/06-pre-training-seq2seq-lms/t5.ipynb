{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2c2e755",
   "metadata": {},
   "source": [
    "# T5\n",
    "\n",
    "- ðŸ“º **Video:** [https://youtu.be/b6KFaT8mK4g](https://youtu.be/b6KFaT8mK4g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c7b925",
   "metadata": {},
   "source": [
    "## Overview\n",
    "- T5 treats every NLP task as text-to-text, enabling multi-task pre-training.\n",
    "- Unify tasks via natural-language prompts and sequence-to-sequence modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42370d87",
   "metadata": {},
   "source": [
    "## Key ideas\n",
    "- **Text-to-text:** both inputs and outputs are text strings.\n",
    "        - **Span corruption:** mask spans with sentinel tokens during pre-training.\n",
    "- **Multi-task mixture:** train on translation, summarization, QA simultaneously.\n",
    "- **Prompting:** task prefix instructs the model what to do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9633a288",
   "metadata": {},
   "source": [
    "## Demo\n",
    "Construct a simple prompt-driven encoder-decoder that maps arithmetic prompts to textual answers, echoing the lecture (https://youtu.be/Mrc4Bcr90VA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9f7e3c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T01:32:12.418441Z",
     "iopub.status.busy": "2025-10-29T01:32:12.418197Z",
     "iopub.status.idle": "2025-10-29T01:32:13.811931Z",
     "shell.execute_reply": "2025-10-29T01:32:13.811634Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  50 | loss 0.2398\n",
      "epoch 100 | loss 0.0310\n",
      "epoch 150 | loss 0.0064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 200 | loss 0.0037\n",
      "Prompt 'add: 4 + 1' -> 6\n",
      "Prompt 'subtract: 7 - 5' -> 3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "prompts = ['add: 1 + 2', 'add: 3 + 3', 'subtract: 5 - 2', 'add: 2 + 4']\n",
    "answers = ['3', '6', '3', '6']\n",
    "\n",
    "characters = set('0123456789') | set(' '.join(prompts + answers))\n",
    "char_vocab = sorted(characters) + ['<pad>', '<s>', '</s>']\n",
    "char_to_id = {c: i for i, c in enumerate(char_vocab)}\n",
    "\n",
    "max_src = max(len(p) for p in prompts) + 2\n",
    "max_tgt = max(len(a) for a in answers) + 2\n",
    "\n",
    "def encode(text, max_len):\n",
    "    ids = [char_to_id['<s>']] + [char_to_id[c] for c in text] + [char_to_id['</s>']]\n",
    "    ids += [char_to_id['<pad>']] * (max_len - len(ids))\n",
    "    return ids\n",
    "\n",
    "src = torch.tensor([encode(p, max_src) for p in prompts])\n",
    "tgt = torch.tensor([encode(a, max_tgt) for a in answers])\n",
    "\n",
    "embed = nn.Embedding(len(char_vocab), 32)\n",
    "encoder = nn.GRU(32, 32, batch_first=True)\n",
    "decoder = nn.GRU(32, 32, batch_first=True)\n",
    "out = nn.Linear(32, len(char_vocab))\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=char_to_id['<pad>'])\n",
    "optimizer = torch.optim.Adam(list(embed.parameters()) + list(encoder.parameters()) + list(decoder.parameters()) + list(out.parameters()), lr=5e-3)\n",
    "\n",
    "for epoch in range(1, 201):\n",
    "    enc_in = embed(src)\n",
    "    _, hidden = encoder(enc_in)\n",
    "    dec_in = embed(tgt[:, :-1])\n",
    "    outputs, _ = decoder(dec_in, hidden)\n",
    "    logits = out(outputs)\n",
    "    loss = criterion(logits.reshape(-1, len(char_vocab)), tgt[:, 1:].reshape(-1))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"epoch {epoch:3d} | loss {loss.item():.4f}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for prompt in ['add: 4 + 1', 'subtract: 7 - 5']:\n",
    "        src_vec = torch.tensor([encode(prompt, max_src)])\n",
    "        enc_in = embed(src_vec)\n",
    "        _, hidden = encoder(enc_in)\n",
    "        dec_input = torch.tensor([[char_to_id['<s>']]])\n",
    "        result = []\n",
    "        hidden_state = hidden\n",
    "        for _ in range(max_tgt):\n",
    "            dec_emb = embed(dec_input)\n",
    "            out_step, hidden_state = decoder(dec_emb, hidden_state)\n",
    "            logits = out(out_step.squeeze(1))\n",
    "            next_id = logits.argmax(dim=-1)\n",
    "            token = next_id.item()\n",
    "            if token == char_to_id['</s>']:\n",
    "                break\n",
    "            if token != char_to_id['<pad>']:\n",
    "                result.append(char_vocab[token])\n",
    "            dec_input = next_id.unsqueeze(1)\n",
    "        print(f\"Prompt '{prompt}' ->\", ''.join(result))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edffcd0d",
   "metadata": {},
   "source": [
    "## Try it\n",
    "- Modify the demo\n",
    "- Add a tiny dataset or counter-example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40c8745",
   "metadata": {},
   "source": [
    "## References\n",
    "- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)\n",
    "- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)\n",
    "- [To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks](https://www.aclweb.org/anthology/W19-4302/)\n",
    "- [GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding](https://arxiv.org/pdf/1804.07461.pdf)\n",
    "- [What Does BERT Look At? An Analysis of BERT's Attention](https://arxiv.org/abs/1906.04341)\n",
    "- [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/pdf/1907.11692.pdf)\n",
    "- [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461)\n",
    "- [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683.pdf)\n",
    "- [UnifiedQA: Crossing Format Boundaries With a Single QA System](https://arxiv.org/abs/2005.00700)\n",
    "- [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/pdf/1508.07909.pdf)\n",
    "- [Byte Pair Encoding is Suboptimal for Language Model Pretraining](https://arxiv.org/pdf/2004.03720.pdf)\n",
    "- [Eisenstein 8.1](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Eisenstein 7.1](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Eisenstein 7.4](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Eisenstein 7.4.1](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Eisenstein 7.3](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [TnT - A Statistical Part-of-Speech Tagger](https://arxiv.org/abs/cs/0003055)\n",
    "- [Enriching the Knowledge Sources Used in a Maximum Entropy Part-of-Speech Tagger](https://www.aclweb.org/anthology/W00-1308/)\n",
    "- [Part-of-Speech Tagging from 97% to 100%: Is It Time for Some Linguistics?](https://link.springer.com/chapter/10.1007/978-3-642-19400-9_14)\n",
    "- [Natural Language Processing with Small Feed-Forward Networks](https://www.aclweb.org/anthology/D17-1309.pdf)\n",
    "- [Eisenstein 10.1-10.2](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Eisenstein 10.3-10.4](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Eisenstein 10.3.1](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Accurate Unlexicalized Parsing](https://www.aclweb.org/anthology/P03-1054/)\n",
    "- [Eisenstein 10.5](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Eisenstein 11.1](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Finding Optimal 1-Endpoint-Crossing Trees](https://www.aclweb.org/anthology/Q13-1002/)\n",
    "- [Eisenstein 11.3](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931dcec1",
   "metadata": {},
   "source": [
    "*Links only; we do not redistribute slides or papers.*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
