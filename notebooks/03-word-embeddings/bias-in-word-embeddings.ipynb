{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9941d50",
   "metadata": {},
   "source": [
    "# Bias in Word Embeddings\n",
    "\n",
    "- üì∫ **Video:** [https://youtu.be/J_227g77Jqg](https://youtu.be/J_227g77Jqg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9204e1c",
   "metadata": {},
   "source": [
    "## Overview\n",
    "An important discussion on how word embeddings can capture biases present in their training corpora. This video explains that because embeddings are learned from real-world text, they may reflect societal stereotypes and biases in that text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3405f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "random.seed(0)\n",
    "CI = os.environ.get('CI') == 'true'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e4b34a",
   "metadata": {},
   "source": [
    "## Key ideas\n",
    "- For example, the notorious analogy result ‚Äúman : computer programmer :: woman : homemaker‚Äù was found in embeddings illustrating a gender bias: the model associated men with technical occupations and women with domestic roles.\n",
    "- The lecture likely quantifies bias using metri like the Word Embedding Association Test (WEAT) and gives examples of gender, ethnic, or racial biases that have been observed (e.g., ‚Äúdoctor‚Äù closer to ‚Äúhe‚Äù and ‚Äúnurse‚Äù to ‚Äúshe‚Äù).\n",
    "- The included references underscore this: Bolukbasi et al.\n",
    "- (2016) explicitly addressed the gender bias in embeddings and proposed a method to ‚Äúdebias‚Äù them by zeroing out the gender direction However, later works like Manzini et al."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72b5ff7",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b9010d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Try the exercises below and follow the linked materials.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c54981",
   "metadata": {},
   "source": [
    "## Try it\n",
    "- Modify the demo\n",
    "- Add a tiny dataset or counter-example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc18813",
   "metadata": {},
   "source": [
    "## References\n",
    "- [Eisenstein 14.5](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Distributed Representations of Words and Phrases and their Compositionality](https://papers.nips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)\n",
    "- [A Scalable Hierarchical Distributed Language Model](https://papers.nips.cc/paper/2008/hash/1e056d2b0ebd5c878c550da6ac5d3724-Abstract.html)\n",
    "- [Neural Word Embedding as Implicit Matrix Factorization](https://papers.nips.cc/paper/2014/file/feab05aa91085b7a8012516bc3533958-Paper.pdf)\n",
    "- [GloVe: Global Vectors for Word Representation](https://www.aclweb.org/anthology/D14-1162/)\n",
    "- [Enriching Word Vectors with Subword Information](https://arxiv.org/abs/1607.04606)\n",
    "- [Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings](https://papers.nips.cc/paper/2016/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf)\n",
    "- [Black is to Criminal as Caucasian is to Police: Detecting and Removing Multiclass Bias in Word Embeddings](https://www.aclweb.org/anthology/N19-1062/)\n",
    "- [Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them](https://www.aclweb.org/anthology/N19-1061/)\n",
    "- [Deep Unordered Composition Rivals Syntactic Methods for Text Classification](https://www.aclweb.org/anthology/P15-1162/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d21d8e",
   "metadata": {},
   "source": [
    "*Links only; we do not redistribute slides or papers.*"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
