{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7be7bdf1",
   "metadata": {},
   "source": [
    "# Applying Embeddings, Deep Averaging Networks\n",
    "\n",
    "- üì∫ **Video:** [https://youtu.be/3pwwdHuH0I4](https://youtu.be/3pwwdHuH0I4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde1c8f1",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This segment demonstrates how learned word embeddings can be applied in an NLP model, using the example of a Deep Averaging Network (DAN). A DAN (Iyyer et al., 2015) is a simple neural architecture for text classification: it takes the average of all the word embeddings in a piece of text (thus ignoring word order entirely, hence ‚Äúunordered composition‚Äù) and then feeds this average vector through one or more feedforward neural layers to predict a label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4fb8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "random.seed(0)\n",
    "CI = os.environ.get('CI') == 'true'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1ef5c5",
   "metadata": {},
   "source": [
    "## Key ideas\n",
    "- The video explains that despite its simplicity - essentially treating the input as a bag of embedded words - a DAN can perform surprisingly well on tasks like sentiment analysis or topic classification Iyyer et al.\n",
    "- found that this simple approach could rival more complex models that incorporated syntax, hence the paper title ‚ÄúDeep Unordered Composition Rivals Syntactic Methods‚Äù.\n",
    "- The lecture likely walks through how a DAN works on a sample sentence: say ‚ÄúThe movie was absolutely fantastic‚Äù.\n",
    "- Each word (‚ÄúThe‚Äù, ‚Äúmovie‚Äù, ‚Äúabsolutely‚Äù, ‚Äúfantastic‚Äù) is replaced by its embedding vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85427f82",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d58f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Try the exercises below and follow the linked materials.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870bf712",
   "metadata": {},
   "source": [
    "## Try it\n",
    "- Modify the demo\n",
    "- Add a tiny dataset or counter-example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3d7058",
   "metadata": {},
   "source": [
    "## References\n",
    "- [Eisenstein 14.5](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Distributed Representations of Words and Phrases and their Compositionality](https://papers.nips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)\n",
    "- [A Scalable Hierarchical Distributed Language Model](https://papers.nips.cc/paper/2008/hash/1e056d2b0ebd5c878c550da6ac5d3724-Abstract.html)\n",
    "- [Neural Word Embedding as Implicit Matrix Factorization](https://papers.nips.cc/paper/2014/file/feab05aa91085b7a8012516bc3533958-Paper.pdf)\n",
    "- [GloVe: Global Vectors for Word Representation](https://www.aclweb.org/anthology/D14-1162/)\n",
    "- [Enriching Word Vectors with Subword Information](https://arxiv.org/abs/1607.04606)\n",
    "- [Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings](https://papers.nips.cc/paper/2016/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf)\n",
    "- [Black is to Criminal as Caucasian is to Police: Detecting and Removing Multiclass Bias in Word Embeddings](https://www.aclweb.org/anthology/N19-1062/)\n",
    "- [Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them](https://www.aclweb.org/anthology/N19-1061/)\n",
    "- [Deep Unordered Composition Rivals Syntactic Methods for Text Classification](https://www.aclweb.org/anthology/P15-1162/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564cf79b",
   "metadata": {},
   "source": [
    "*Links only; we do not redistribute slides or papers.*"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
