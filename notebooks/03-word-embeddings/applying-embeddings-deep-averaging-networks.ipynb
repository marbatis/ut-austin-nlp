{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7be7bdf1",
   "metadata": {},
   "source": [
    "# Applying Embeddings, Deep Averaging Networks\n",
    "\n",
    "- ðŸ“º **Video:** [https://youtu.be/3pwwdHuH0I4](https://youtu.be/3pwwdHuH0I4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde1c8f1",
   "metadata": {},
   "source": [
    "## Overview\n",
    "- Explore deep averaging networks (DANs) that compose word embeddings by averaging then feeding through layers.\n",
    "- Appreciate how pre-trained embeddings transfer to downstream classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1ef5c5",
   "metadata": {},
   "source": [
    "## Key ideas\n",
    "- **Averaging:** simple pooling of embeddings provides surprisingly strong baselines.\n",
    "- **Nonlinear layers:** additional dense layers refine averaged vectors.\n",
    "- **Transfer learning:** initialize with pre-trained embeddings to reduce data requirements.\n",
    "- **Regularization:** dropout applied to embeddings combats co-adaptation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85427f82",
   "metadata": {},
   "source": [
    "## Demo\n",
    "Average pre-trained-like embeddings from our toy corpus and pass them through a small neural reader to classify sentiment, mirroring the lecture (https://youtu.be/USkdJfi4V64)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17d58f1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T01:27:53.946090Z",
     "iopub.status.busy": "2025-10-29T01:27:53.945856Z",
     "iopub.status.idle": "2025-10-29T01:27:54.742267Z",
     "shell.execute_reply": "2025-10-29T01:27:54.741963Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000         2\n",
      "           1      0.750     1.000     0.857         6\n",
      "\n",
      "    accuracy                          0.750         8\n",
      "   macro avg      0.375     0.500     0.429         8\n",
      "weighted avg      0.562     0.750     0.643         8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcelosilveira/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/marcelosilveira/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/marcelosilveira/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "corpus = [\n",
    "    'she is a skilled doctor and compassionate leader',\n",
    "    'he is a brilliant engineer and creative designer',\n",
    "    'the nurse offered patient support and kindness',\n",
    "    'the manager coordinated the project with precision',\n",
    "    'artists create inspiring work with emotion and style',\n",
    "    'scientists test hypotheses with rigorous experiments',\n",
    "    'teachers guide students with patience and care',\n",
    "    'the programmer solved complex problems quickly'\n",
    "]\n",
    "\n",
    "vocab = sorted(set(' '.join(corpus).split()))\n",
    "word_to_id = {word: idx for idx, word in enumerate(vocab)}\n",
    "emb_dim = 10\n",
    "rng = np.random.default_rng(42)\n",
    "embeddings = rng.normal(scale=0.3, size=(len(vocab), emb_dim))\n",
    "\n",
    "train_texts = [\n",
    "    'compassionate nurse offered support',\n",
    "    'creative artist delivered stunning work',\n",
    "    'patient teacher inspired students',\n",
    "    'the experiment succeeded with rigor',\n",
    "    'the engineer solved complex design',\n",
    "    'kind doctor reassured everyone',\n",
    "    'the manager mismanaged the schedule',\n",
    "    'the designer delivered a clumsy layout'\n",
    "]\n",
    "labels = [1, 1, 1, 1, 1, 1, 0, 0]\n",
    "\n",
    "features = []\n",
    "for sentence in train_texts:\n",
    "    ids = [word_to_id[word] for word in sentence.split() if word in word_to_id]\n",
    "    if not ids:\n",
    "        features.append(np.zeros(emb_dim))\n",
    "        continue\n",
    "    avg = embeddings[ids].mean(axis=0)\n",
    "    hidden = np.tanh(avg)\n",
    "    features.append(hidden)\n",
    "\n",
    "features = np.vstack(features)\n",
    "clf = LogisticRegression(max_iter=2000, random_state=0)\n",
    "clf.fit(features, labels)\n",
    "\n",
    "pred = clf.predict(features)\n",
    "print(classification_report(labels, pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870bf712",
   "metadata": {},
   "source": [
    "## Try it\n",
    "- Modify the demo\n",
    "- Add a tiny dataset or counter-example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3d7058",
   "metadata": {},
   "source": [
    "## References\n",
    "- [Eisenstein 14.5](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Distributed Representations of Words and Phrases and their Compositionality](https://papers.nips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)\n",
    "- [A Scalable Hierarchical Distributed Language Model](https://papers.nips.cc/paper/2008/hash/1e056d2b0ebd5c878c550da6ac5d3724-Abstract.html)\n",
    "- [Neural Word Embedding as Implicit Matrix Factorization](https://papers.nips.cc/paper/2014/file/feab05aa91085b7a8012516bc3533958-Paper.pdf)\n",
    "- [GloVe: Global Vectors for Word Representation](https://www.aclweb.org/anthology/D14-1162/)\n",
    "- [Enriching Word Vectors with Subword Information](https://arxiv.org/abs/1607.04606)\n",
    "- [Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings](https://papers.nips.cc/paper/2016/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf)\n",
    "- [Black is to Criminal as Caucasian is to Police: Detecting and Removing Multiclass Bias in Word Embeddings](https://www.aclweb.org/anthology/N19-1062/)\n",
    "- [Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them](https://www.aclweb.org/anthology/N19-1061/)\n",
    "- [Deep Unordered Composition Rivals Syntactic Methods for Text Classification](https://www.aclweb.org/anthology/P15-1162/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564cf79b",
   "metadata": {},
   "source": [
    "*Links only; we do not redistribute slides or papers.*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
