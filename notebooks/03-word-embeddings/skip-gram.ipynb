{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3195711a",
   "metadata": {},
   "source": [
    "# Skip-gram\n",
    "\n",
    "- ðŸ“º **Video:** [https://youtu.be/hznxqCIrzSQ](https://youtu.be/hznxqCIrzSQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11958d39",
   "metadata": {},
   "source": [
    "## Overview\n",
    "- Study the skip-gram objective that predicts surrounding context words from a center word.\n",
    "- Understand negative sampling as an efficient approximation to softmax normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918fd1de",
   "metadata": {},
   "source": [
    "## Key ideas\n",
    "- **Predictive training:** maximize probability of observed (center, context) pairs.\n",
    "- **Negative sampling:** sample noise words to contrast with true context words.\n",
    "- **Embeddings:** input and output embeddings jointly learn semantic regularities.\n",
    "- **Optimization:** stochastic gradient updates on mini-batches scale to large corpora."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9ccca1",
   "metadata": {},
   "source": [
    "## Demo\n",
    "Train a tiny skip-gram model with negative sampling on a toy corpus to illustrate the gradient updates described in the lecture (https://youtu.be/4P_yGJvqMeI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15e55c10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T01:27:57.195138Z",
     "iopub.status.busy": "2025-10-29T01:27:57.194876Z",
     "iopub.status.idle": "2025-10-29T01:28:00.980166Z",
     "shell.execute_reply": "2025-10-29T01:28:00.979809Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 200 | loss 4.3505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bg/kk27r5597d90__bcbm9dkblc0000gn/T/ipykernel_9138/3456499083.py:40: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 400 | loss 17.9440\n",
      "epoch 600 | loss 17.8067\n",
      "epoch 800 | loss 17.7043\n",
      "\n",
      "Nearest neighbors after training:\n",
      "doctor -> [('creative', 0.9862556976884768), ('compassionate', 0.9844193319770509), ('brilliant', 0.974811414015661), ('care', 0.9734487544990958)]\n",
      "engineer -> [('designer', 0.9916436433017531), ('kindness', 0.9669990182026375), ('leader', 0.9637092849382919), ('patience', 0.9400564512125047)]\n",
      "teachers -> [('test', 0.9393482001183809), ('precision', 0.8593904508848271), ('work', 0.8400085169632004), ('is', 0.817506675612962)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "corpus = [\n",
    "    'she is a skilled doctor and compassionate leader',\n",
    "    'he is a brilliant engineer and creative designer',\n",
    "    'the nurse offered patient support and kindness',\n",
    "    'the manager coordinated the project with precision',\n",
    "    'artists create inspiring work with emotion and style',\n",
    "    'scientists test hypotheses with rigorous experiments',\n",
    "    'teachers guide students with patience and care',\n",
    "    'the programmer solved complex problems quickly'\n",
    "]\n",
    "\n",
    "tokens = [word for sentence in corpus for word in sentence.split()]\n",
    "vocab = sorted(set(tokens))\n",
    "word_to_id = {word: idx for idx, word in enumerate(vocab)}\n",
    "id_to_word = {idx: word for word, idx in word_to_id.items()}\n",
    "counts = Counter(tokens)\n",
    "distribution = np.array([counts[word] for word in vocab], dtype=float)\n",
    "noise_dist = (distribution ** 0.75) / np.sum(distribution ** 0.75)\n",
    "\n",
    "window = 2\n",
    "embedding_dim = 8\n",
    "rng = np.random.default_rng(1)\n",
    "W_in = rng.normal(scale=0.1, size=(len(vocab), embedding_dim))\n",
    "W_out = rng.normal(scale=0.1, size=(len(vocab), embedding_dim))\n",
    "\n",
    "pairs = []\n",
    "for sentence in corpus:\n",
    "    words = sentence.split()\n",
    "    for i, word in enumerate(words):\n",
    "        target = word_to_id[word]\n",
    "        for j in range(max(0, i - window), min(len(words), i + window + 1)):\n",
    "            if i == j:\n",
    "                continue\n",
    "            pairs.append((target, word_to_id[words[j]]))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "lr = 0.05\n",
    "num_neg = 3\n",
    "\n",
    "for epoch in range(1, 801):\n",
    "    total_loss = 0.0\n",
    "    rng.shuffle(pairs)\n",
    "    for target, context in pairs:\n",
    "        target_vec = W_in[target]\n",
    "        context_vec = W_out[context]\n",
    "        score = target_vec @ context_vec\n",
    "        prob = sigmoid(score)\n",
    "        grad = (prob - 1)\n",
    "        W_in[target] -= lr * grad * context_vec\n",
    "        W_out[context] -= lr * grad * target_vec\n",
    "        total_loss -= np.log(prob + 1e-8)\n",
    "\n",
    "        neg_ids = rng.choice(len(vocab), size=num_neg, p=noise_dist)\n",
    "        for neg in neg_ids:\n",
    "            neg_vec = W_out[neg]\n",
    "            neg_score = target_vec @ neg_vec\n",
    "            neg_prob = sigmoid(neg_score)\n",
    "            grad_neg = neg_prob\n",
    "            W_in[target] -= lr * grad_neg * neg_vec\n",
    "            W_out[neg] -= lr * grad_neg * target_vec\n",
    "            total_loss -= np.log(1 - neg_prob + 1e-8)\n",
    "    if epoch % 200 == 0:\n",
    "        print(f\"epoch {epoch:3d} | loss {total_loss/len(pairs):.4f}\")\n",
    "\n",
    "embeddings = W_in\n",
    "\n",
    "def similar(word, top_k=4):\n",
    "    idx = word_to_id[word]\n",
    "    vec = embeddings[idx]\n",
    "    sims = embeddings @ vec / (np.linalg.norm(embeddings, axis=1) * np.linalg.norm(vec) + 1e-8)\n",
    "    ranking = sims.argsort()[::-1]\n",
    "    return [(id_to_word[i], sims[i]) for i in ranking if i != idx][:top_k]\n",
    "\n",
    "print()\n",
    "print('Nearest neighbors after training:')\n",
    "for word in ['doctor', 'engineer', 'teachers']:\n",
    "    print(word, '->', similar(word))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99ba456",
   "metadata": {},
   "source": [
    "## Try it\n",
    "- Modify the demo\n",
    "- Add a tiny dataset or counter-example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09307d88",
   "metadata": {},
   "source": [
    "## References\n",
    "- [Eisenstein 14.5](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "- [Distributed Representations of Words and Phrases and their Compositionality](https://papers.nips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)\n",
    "- [A Scalable Hierarchical Distributed Language Model](https://papers.nips.cc/paper/2008/hash/1e056d2b0ebd5c878c550da6ac5d3724-Abstract.html)\n",
    "- [Neural Word Embedding as Implicit Matrix Factorization](https://papers.nips.cc/paper/2014/file/feab05aa91085b7a8012516bc3533958-Paper.pdf)\n",
    "- [GloVe: Global Vectors for Word Representation](https://www.aclweb.org/anthology/D14-1162/)\n",
    "- [Enriching Word Vectors with Subword Information](https://arxiv.org/abs/1607.04606)\n",
    "- [Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings](https://papers.nips.cc/paper/2016/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf)\n",
    "- [Black is to Criminal as Caucasian is to Police: Detecting and Removing Multiclass Bias in Word Embeddings](https://www.aclweb.org/anthology/N19-1062/)\n",
    "- [Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them](https://www.aclweb.org/anthology/N19-1061/)\n",
    "- [Deep Unordered Composition Rivals Syntactic Methods for Text Classification](https://www.aclweb.org/anthology/P15-1162/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f95a88",
   "metadata": {},
   "source": [
    "*Links only; we do not redistribute slides or papers.*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
